{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import Tensor\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Auxiliary Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear:\n",
    "    \"\"\"\n",
    "    An class that contains objects which only store layar's in/out connections dimension\n",
    "    \n",
    "    input_s:\n",
    "        dim_in      :  the input_ dimension of fully connected layer\n",
    "        dim_out     :  the output_ dimension of fully connected layer\n",
    "        \n",
    "    returns:\n",
    "        Nothing \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self,dim_in, dim_out):\n",
    "        self.input_ = dim_in\n",
    "        self.output_ = dim_out          \n",
    "    # TODO: The linear is really wierd thing... all we get here is already in the upper class. we may omit this somehow\n",
    "\n",
    "#---------------------------------------------------------------------------------------------\n",
    "        \n",
    "def Activation(code,input_):\n",
    "    \"\"\"\n",
    "    A class that specify the needed activation with respect to the following code conversion\n",
    "        0: Relu(x)\n",
    "        1: Tanh(x)\n",
    "        2: Sigmoid(x)\n",
    "    \n",
    "    This class works as functional package of pytorch\n",
    "    \n",
    "    input_s:\n",
    "        code        :  the code for each activation (0,1,2)\n",
    "        input__tensor:  the input_ tensor\n",
    "        \n",
    "    returns:\n",
    "        result      :  the output_ of requested activation function with the same shape as input_ tensor\n",
    "    \"\"\"\n",
    "    result = Tensor(input_.shape)\n",
    "    # Relu\n",
    "    if code ==0:\n",
    "        result = input_ - (input_<0).float()*input_\n",
    "    # Tanh\n",
    "    elif code ==1:\n",
    "        result = torch.tanh(input_)\n",
    "    # Sig\n",
    "    elif code ==2:\n",
    "        result = 1.0/(1 + torch.exp(-input_))\n",
    "    # linear\n",
    "    elif code ==3:\n",
    "        result = input_\n",
    "    # error\n",
    "    else: raise ValueError('Unknown Code For Activation')\n",
    "        \n",
    "    return result \n",
    "\n",
    "#---------------------------------------------------------------------------------------------\n",
    "\n",
    "def dActivation(code,input_):\n",
    "    \"\"\"\n",
    "    A class that specify the needed derivative of activation with the same encoding convenstion\n",
    "        0: dRelu(x)\n",
    "        1: dTanh(x)\n",
    "        2: dSigmoid(x)\n",
    "    \n",
    "    This class works as functional package of pytorch\n",
    "    \n",
    "    input_s:\n",
    "        code        :  the code for each activation (0,1,2)\n",
    "        input__tensor:  the input_ tensor\n",
    "        \n",
    "    returns:\n",
    "        result      :  the output_ of requested activation function with the same shape as input_ tensor\n",
    "    \"\"\"\n",
    "    result = Tensor.new(input_)\n",
    "    # dRelu\n",
    "    if code ==0:\n",
    "        result = Tensor(input_.shape).fill_(1.0) - (input_<=0).float()*Tensor(input_.shape).fill_(1.0)\n",
    "    # dTanh\n",
    "    elif code ==1:\n",
    "        result = 1-(torch.tanh(input_))**2\n",
    "    # dSig\n",
    "    elif code ==2:\n",
    "        result = Activation(code,input_)*(1-Activation(code,input_))\n",
    "    # linear\n",
    "    elif code ==3:\n",
    "        result = Tensor(input_.shape).fill_(1.0)\n",
    "    else: raise ValueError('Unknown Code For derivative of Activation')\n",
    "    \n",
    "    return result \n",
    "\n",
    "#---------------------------------------------------------------------------------------------\n",
    "    \n",
    "class PassPar:    # change the name to: \"ForwardAns\n",
    "    \"\"\"\n",
    "    This class keeps track of all the variables produced in forward pass of some layer. i.e, x and s.\n",
    "    \n",
    "    inputs:\n",
    "        N           :  number of data\n",
    "        dim_in      :  the input_ dimension of fully connected layer\n",
    "        dim_out     :  the output_ dimension of fully connected layer\n",
    "        \n",
    "    returns:\n",
    "        Nothing\n",
    "    \"\"\"\n",
    "    def __init__(self,dim_in, dim_out, N):\n",
    "        self.s = Tensor(N,dim_out).fill_(0)           # s after each layer\n",
    "        self.x = Tensor(N,dim_out).fill_(0)           # x after each layer:   x = Activation (s)\n",
    "        self.db = Tensor(N, dim_out,1).fill_(0)       # dL/db for each input\n",
    "        self.dw = Tensor(N, dim_out,dim_in).fill_(0)  # dL/dw for each input\n",
    "        self.ds = Tensor(N, dim_out).fill_(0)         # dL/ds for each input\n",
    "        self.dx = Tensor(N, dim_out).fill_(0)         # dL/dx for each input\n",
    "        \n",
    "#---------------------------------------------------------------------------------------------\n",
    "\n",
    "class Param:\n",
    "    \"\"\"\n",
    "    This class contains parameters of each layer. We initialize them in constructor.\n",
    "    \n",
    "    inputs:\n",
    "        dim_in      :  the input dimension of fully connected layer\n",
    "        dim_out     :  the output dimension of fully connected layer\n",
    "        b           :  bias vector\n",
    "        w           :  weight matrix\n",
    "        db          :  grad wrt bias\n",
    "        dw          :  grad wrt weight matrix\n",
    "    modules:\n",
    "        data        :  returns the parameters as one tensor \n",
    "        grad        :  returns the grad of Loss wrt to parameter as one tensor\n",
    "    \"\"\"\n",
    "    def __init__(self, dim_in, dim_out):\n",
    "        self.dim_in  = dim_in\n",
    "        self.dim_out = dim_out\n",
    "        self.b = Tensor(dim_out,1).fill_(0)          # bias of each layer\n",
    "        self.w = Tensor(dim_out,dim_in).normal_()    # w of each layer\n",
    "        self.db = Tensor(dim_out,1).fill_(0)         # dL/dbias of each layer\n",
    "        self.dw = Tensor(dim_out,dim_in).fill_(0)    # dL/dw of each layer\n",
    "    \n",
    "    \n",
    "    def data(self):\n",
    "        return (torch.cat((self.w, self.b),1))\n",
    "    \n",
    "    def grad(self):\n",
    "        \"\"\"        print (self.dw.shape)\n",
    "        print (self.db.shape)\n",
    "        print (self.w.shape)\n",
    "        print (self.b.shape)\"\"\"\n",
    "        return (torch.cat((self.dw, self.db),1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LossMSE and Error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(v, t):\n",
    "    l_ = torch.sum(torch.pow(v-t,2))/(len(v))\n",
    "    return l_\n",
    "\n",
    "def dloss(v,t):\n",
    "    return 2.*(v-t)/(len(v))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def error(Prediction, target_):\n",
    "    # if the score of two classes are equal, we consider it as a misclassification\n",
    "    e = 0\n",
    "    for n in list(range(len(target_))):\n",
    "        if target_[n,0]>target_[n,1]:                  # class 0\n",
    "            if Prediction[n,0]<=Prediction[n,1]: e +=1\n",
    "        else:                                      # class 1\n",
    "            if Prediction[n,0]>=Prediction[n,1]: e +=1\n",
    "    print ('error is {}%'.format(1./len(target_)*e*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Network Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net:\n",
    "    \"\"\"\n",
    "    The network class. It has the following methods:\n",
    "        param      :  returns the parameter which is asked for. Not the data! The object... \n",
    "                        Data is accessible through object.data method)\n",
    "        make_arch  :  makes the architecture of the network by taking a sequential list of [fc1,act1,fc2,act2,...]\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, seq, X,Y,mini_batch_size=None):\n",
    "        \n",
    "        self.n_layer =len(seq)//2                           # number of layer  ((*/2) is because of the activations...)\n",
    "        self.N = len(X)                                     # nb of batch\n",
    "        if mini_batch_size!=None: \n",
    "            self.N = mini_batch_size\n",
    "            \n",
    "        self.param_list    = list(range(self.n_layer))      # Stores parameters of each layer (W,b) and their grads  \n",
    "        self.pass_list     = list(range(self.n_layer+1))    # Stores parameters evaluted during forward/backward pass\n",
    "        \n",
    "        self.act_list = list(range(self.n_layer))           # stores the requested activation functions in codes. Elements are \"0\",\"1\" or \"2\"\n",
    "        self.make_arch(seq)                                 # makes the architecture based on the the list \"seq\"\n",
    "        \n",
    "\n",
    "    def make_arch(self,seq):    # makeArch\n",
    "        \"\"\"\n",
    "        This function fills param_list and act_list and also, evaluates number of layers.\n",
    "        \n",
    "        input:\n",
    "            seq : a list that contains both activation and layers in a sequential manner\n",
    "                  Example: [Linear(3,5), 'relu', Linear(5,64), 'Sig', Linear(64,1), 'Tanh']\n",
    "        \"\"\"\n",
    "        \n",
    "        seq_len = len(seq)                  # number of layer *2 (because of the activations...)\n",
    "        for layer in list(range(0,seq_len,2)):\n",
    "             \n",
    "            # seq[layer] is an instance of object \"Linear\". Here we get the in/out dim of the layer\n",
    "            dim_in, dim_out = seq[layer].input_ , seq[layer].output_ \n",
    "            \n",
    "            # making a new Param instance and adding it to the param_list\n",
    "            self.param_list[int(layer/2)]= Param(dim_in, dim_out) \n",
    "            \n",
    "            # activation recognition : encode activations in \"act_list\"\n",
    "            if seq[layer+1]=='relu':\n",
    "                self.act_list.append(0)\n",
    "            elif seq[layer+1]=='tanh':\n",
    "                self.act_list.append(1)\n",
    "            elif seq[layer+1]=='sig':\n",
    "                self.act_list.append(2)\n",
    "            elif seq[layer+1]=='lin':\n",
    "                self.act_list.append(3)\n",
    "            else: raise ValueError('Unknown Activation')\n",
    "                    \n",
    "    \n",
    "    def forward(self,X): \n",
    "        \"\"\"\n",
    "        This method evaluates the forward pass and returns the values. This function is written such that it take a\n",
    "        batch input and returns the forward pass of the batch.\n",
    "        \n",
    "        input: \n",
    "            X    :   a tensor of size(B, d_in) \n",
    "            \n",
    "        returns:\n",
    "            s    :   a tensor of size(B, d_out) \n",
    "            x    :   a tensor of size(B, d_out) \n",
    "        \"\"\"\n",
    "        \n",
    "        x=X\n",
    "        for layer, prm in enumerate(self.param_list):   # layer = [0,1,2,...,nb_layer -1]  ;  prm = param_list[layer]\n",
    "            s = (x.mm(prm.w.t()) + prm.b.t())           # written consistant for batch :  s = (Wx+b).t()  (N,d_out) \n",
    "            x = Activation(self.act_list[layer], s)     # size = (N,d_out)       \n",
    "        return x,s\n",
    "    \n",
    "        \"\"\"\n",
    "        Hint:\n",
    "            prm.w.shape = (d_out, d_in)\n",
    "            prm.b.shpae = (d_out, 1)\n",
    "            X.shape     = (N,d_in)\n",
    "            x.shape     = (N,d_out)\n",
    "            s.shape     = (N.d_out)\n",
    "        \"\"\"\n",
    "    \n",
    "    \n",
    "    def backward (self,X,Y):\n",
    "        \"\"\"\n",
    "        This method fills pass_list by constructing instances of PassPar.The object PassPar, contains\n",
    "        (s,x) and also the gradients with respect to all parameteres of each layer (dL/dw, dL/db, dL/ds, dL/dx)\n",
    "        during a froward and backwar pass.\n",
    "        \n",
    "        These gradients are responsible for all batch data. The sum of all parameters determines the\n",
    "        total gradient of the batch. This summation is stored in the respective attributes of param_list that \n",
    "        contains Param objects.\n",
    "        \n",
    "        This function return the values of MSE loss during each pass.\n",
    "        \n",
    "        input:\n",
    "            X        :   training set of size (B,d)\n",
    "            Y        :   target set of size (B,d)\n",
    "            \n",
    "        returns:\n",
    "            loss     :   value of MSE loss\n",
    "        \"\"\"\n",
    "        \n",
    "        # add the input X to the pass list\n",
    "        self.pass_list[0]= PassPar(self.param_list[0].dim_in, self.param_list[0].dim_in, self.N)    # Note that dim = dim_in for inputs\n",
    "        self.pass_list[0].x =X    # x0\n",
    "        self.pass_list[0].s =X    # s0 is set to be x0\n",
    "        \n",
    "        \n",
    "        # this computes forward pass and saves s and x\n",
    "        x=X\n",
    "        for layer, prm in enumerate(self.param_list): # layer = [0,1,2,...,nb_layer -1]  ; prm = param_list[layer]  \n",
    "            \n",
    "            \"\"\"\n",
    "            hint: PassPar has following attributes:\n",
    "                s  = Tensor(N, dim_out)           \n",
    "                x  = Tensot(N, dim_out)\n",
    "            \"\"\"\n",
    "            \n",
    "            self.pass_list[layer+1]= PassPar(prm.dim_in, prm.dim_out, self.N)  # instantiating pass parameters\n",
    "            \n",
    "            #print('hey there and shape of pass_list[layer+1].dw is {}'.format(self.pass_list[layer+1].dw.shape))\n",
    "            \n",
    "            s = (x.mm(prm.w.t()) + prm.b.t())         # consistant with batch. s.shape = (N,d_out)\n",
    "            self.pass_list[layer+1].s = s             \n",
    "            x = Activation(self.act_list[layer], s)    \n",
    "            self.pass_list[layer+1].x = x\n",
    "         \n",
    "        \n",
    "        # this computes backward\n",
    "        for layer in list(range (self.n_layer,-1,-1)): # layer=[nb_layer, nb_layer-1 , ..., 1]\n",
    "            \n",
    "            \"\"\"\n",
    "            hint: PassPar also has the following attributes:\n",
    "                db = Tensor(N, dim_out,1)         # dL/db\n",
    "                dw = Tensor(N, dim_out,dim_in)    # dL/dw\n",
    "                ds = Tensor(N, dim_out)           # dL/ds\n",
    "                dx = Tensor(N, dim_out)           # dL/dx\n",
    "            \"\"\"\n",
    "            \n",
    "            # dl/dx : size = N,d_out\n",
    "            if layer == self.n_layer:\n",
    "                self.pass_list[layer].dx = dloss(x,Y)   \n",
    "            else:\n",
    "                self.pass_list[layer].dx = self.pass_list[layer+1].ds.mm(self.param_list[layer].w)\n",
    "            \"\"\"\n",
    "            hint:\n",
    "                dloss(x,Y)                 = (N, d_out_{last_layer})\n",
    "                pass_list[layer+1].ds      = (N, d_out_{layer+1})\n",
    "                param_list[layer].w        = (d_out_{layer+1}, d_in_{layer+1}) = (d_out_{layer+1}, d_out_{layer}) \n",
    "            \"\"\"\n",
    "                \n",
    "                \n",
    "            #dl/ds : size = N,d_out\n",
    "            self.pass_list[layer].ds = self.pass_list[layer].dx * dActivation(self.act_list[layer-1], self.pass_list[layer].s ) \n",
    "            \"\"\"\n",
    "            hint:\n",
    "                self.act_list[layer-1] :  activation type of layer = layer (0,1,2 or 3)\n",
    "                pass_list[layer].dx    =  (N, d_out)\n",
    "                pass_list[layer].s     =  (N, d_out)\n",
    "                pass_list[layer].ds    =  (N, d_out)\n",
    "                dActivation(code, s)   =  (N, d_out)\n",
    "            \"\"\"\n",
    "            \n",
    "            \n",
    "            #dl/dw : size = (N, d_out, d_in)\n",
    "            ds_unsq = self.pass_list[layer].ds.unsqueeze(1)    \n",
    "            x_unsq  = self.pass_list[layer-1].x.unsqueeze(2)\n",
    "            self.pass_list[layer].dw =  (x_unsq * ds_unsq).transpose(1,2)         \n",
    "            \"\"\"\n",
    "            hint:\n",
    "                pass_list[layer].ds        = (N, d_out)\n",
    "                self.pass_list[layer-1].x  = (N, d_in)\n",
    "                ds_unsq                    = (N, 1, d_out)\n",
    "                dx_unsq                    = (N, d_in, 1)\n",
    "                pass_list[layer].dw        = (N, d_out, d_in)\n",
    "            \"\"\"\n",
    "            \n",
    "            \n",
    "            #dl/db : size = (N, d_out)\n",
    "            self.pass_list[layer].db = self.pass_list[layer].ds\n",
    "            \n",
    "            \"\"\"\n",
    "            hint:\n",
    "                pass_list[layer].ds        = (N, d_out)\n",
    "                pass_list[layer].db        = (N, d_out)\n",
    "            \"\"\"\n",
    "\n",
    "        # summing all batch grads\n",
    "        for layer in list (range(0,self.n_layer)):\n",
    "            self.param_list[layer].db = self.pass_list[layer+1].db.sum(0).unsqueeze(1)  \n",
    "            self.param_list[layer].dw = self.pass_list[layer+1].dw.sum(0)\n",
    "            #print('hey there in sum and shape of pass_list[layer+1].dw is {}'.format(self.pass_list[layer+1].dw.shape))\n",
    "    \n",
    "    \n",
    "    def parameters (self):\n",
    "        return self.param_list\n",
    "\n",
    "    \"\"\"    \n",
    "    def train_thresh(self,X,Y,eta,thresh):\n",
    "        L0=0\n",
    "        counter =0\n",
    "        while counter<10000:\n",
    "            self.backward(X,Y)\n",
    "            L= loss()\n",
    "            if (abs(L-L0)<thresh):\n",
    "                break\n",
    "            for p in self.parameters():\n",
    "                p.b -= eta* p.db\n",
    "                p.w -= eta* p.dw\n",
    "            \n",
    "            L0=L\n",
    "            if counter%50==0: print('counter: {}\\tLoss value is {}'.format(counter, L))\n",
    "            counter += 1\n",
    "    \"\"\"        \n",
    "    def train(self, X, Y, eta, epoch):\n",
    "        \n",
    "        mini_batch_size = self.N   # N if no batch size is provided at initialization of network, else it is already mini_batch_size\n",
    "        e  = 0\n",
    "        while e< epoch:\n",
    "            L_mini = 0\n",
    "            for batch in list(range (0, train_input.size(0), mini_batch_size)):\n",
    "                \n",
    "                # Note that here we feed the backward by mini_train and mini_target defined as below: \n",
    "                #\n",
    "                #     mini_train = train_input.narrow(0, b, mini_batch_size)\n",
    "                #     mini_target= train_target.narrow(0, b, mini_batch_size)\n",
    "                \n",
    "                self.backward(train_input.narrow(0, batch, mini_batch_size) ,\n",
    "                              train_target.narrow(0, batch, mini_batch_size) )     \n",
    "                \n",
    "                for p in self.parameters():\n",
    "                    p.b -= eta* p.db\n",
    "                    p.w -= eta* p.dw\n",
    "                \n",
    "                L_mini += loss(self.pass_list[-1].x, train_target.narrow(0, batch, mini_batch_size))\n",
    "                    \n",
    "            L=L_mini\n",
    "            if e%50==0: print('epoch: {}\\tLoss value is {}'.format(e, L))\n",
    "            e+=1\n",
    "     \n",
    "    \"\"\"\n",
    "    We don't need to use this method. Since all the gradients are not accumelated in our code.\n",
    "            \n",
    "    def zero_grad(self):\n",
    "    \n",
    "        for g in self.grad_list:\n",
    "            g.db.fill_(0)\n",
    "            g.dw.fill_(0)\n",
    "            g.dx.fill_(0)\n",
    "            g.ds.fill_(0)\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating a training and a test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train set\n",
    "\n",
    "N_train=10000             # number of points\n",
    "x=torch.rand(N_train)    # x pos\n",
    "y=torch.rand(N_train)    # y pos\n",
    "\n",
    "train_input = torch.cat((x.unsqueeze_(1),y.unsqueeze_(1)),1)       # input batch\n",
    "train_target =Tensor(N_train,2)              # target batch\n",
    "\n",
    "#r=1/(2*np.pi) #r^2\n",
    "r=1\n",
    "\n",
    "# making a hot vector target\n",
    "train_target[:,0] = (x**2 + y**2 <=r).float() - ( x**2 + y**2 > r).float()   # 1 if inside circle, else -1\n",
    "train_target[:,1] = (x**2 + y**2 > r).float() - ( x**2 + y**2 <=r).float()   # -1 if inside circle else 1\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Test set\n",
    "\n",
    "N_test=1000              # number of points\n",
    "x=torch.rand(N_test)    # x pos\n",
    "y=torch.rand(N_test)    # y pos\n",
    "\n",
    "test_input = torch.cat((x.unsqueeze_(1),y.unsqueeze_(1)),1)       # input batch\n",
    "test_target =Tensor(N_test,2)              # target batch\n",
    "\n",
    "#r=1/(2*np.pi) #r^2\n",
    "r=1\n",
    "\n",
    "# making a hot vector target\n",
    "test_target[:,0] = (x**2 + y**2 <=r).float() - ( x**2 + y**2 > r).float()   # 1 if inside circle, else -1\n",
    "test_target[:,1] = (x**2 + y**2 > r).float() - ( x**2 + y**2 <=r).float()   # -1 if inside circle else 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.01\n",
    "epoch = 1000\n",
    "batch_size = 50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training a simple network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq = [Linear(2,10),'sig',Linear(10,20),'relu',Linear(20,10),'relu',Linear(10,2),'tanh']\n",
    "model = Net(seq, train_input, train_target, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameters before training\n",
      "\n",
      "\n",
      "Parameters of layer in the (w,b) format \n",
      "\n",
      "-0.4135 -2.2940  0.0000\n",
      "-0.5826 -0.2140  0.0000\n",
      "-1.4465 -0.6499  0.0000\n",
      "-0.8583 -0.3719  0.0000\n",
      "-0.2567  1.6391  0.0000\n",
      "-0.8026 -0.9842  0.0000\n",
      " 0.3631  0.3333  0.0000\n",
      "-1.2274 -0.5868  0.0000\n",
      "-0.7886  0.2846  0.0000\n",
      " 0.7370  0.0801  0.0000\n",
      "[torch.FloatTensor of size 10x3]\n",
      "\n",
      "\n",
      "\n",
      "Parameters of layer in the (w,b) format \n",
      "\n",
      "\n",
      "Columns 0 to 9 \n",
      "-0.8951  1.3926  1.0456  0.6050 -0.1991  0.4710 -0.8784 -1.0665 -0.9049  0.1054\n",
      " 0.8866 -1.1448 -0.1763 -1.8596 -0.0878  0.8653  0.6362  0.1321 -0.6461  1.4947\n",
      " 0.1574 -0.5391 -0.9920  0.3360  2.1402  0.6468  0.9763  1.2540  0.7675 -0.3045\n",
      "-1.3645  0.7671  0.8102  0.6664 -1.2551  1.0090 -0.1184 -0.9454 -2.7526  0.1086\n",
      " 1.0101  1.3345  0.6570 -0.0185  0.1281  0.7455  0.2341  0.4798 -0.2629 -1.2186\n",
      " 1.6591  1.9904 -1.2563 -0.4836 -0.0747 -0.2568 -0.8441  0.4144 -0.3495 -1.2126\n",
      " 0.7089  2.1417  0.4376  0.5361  0.3941  0.1793  0.3001  0.4433 -0.1051 -1.0083\n",
      " 0.1610 -0.4964  1.3073  0.6303  0.0880 -0.0018 -1.7361  0.6965  0.5794 -0.3885\n",
      "-1.0133 -0.4030 -1.1582  0.8197  0.1333  0.3127 -0.2597 -0.3187  0.5441 -0.5236\n",
      " 0.0347 -1.0142 -1.5014  0.8920 -0.0594 -0.5783 -0.1304  1.3696 -0.2035  0.1256\n",
      " 3.0978  0.8311  1.8206 -1.9508  0.5828 -2.1428  0.6132  2.2078 -0.9460  0.2180\n",
      " 0.1455  0.5357  0.8443 -0.6890 -0.4162  0.7897  0.6822  1.8250  1.8868  0.0819\n",
      " 0.4742 -1.5897  0.1263  0.5880 -0.0170 -0.1132 -0.2586 -1.2987 -1.2650  0.3208\n",
      " 0.5288  0.9403 -0.8324 -0.2340 -0.1803  1.1727  0.5259  0.3777  0.4373 -0.2178\n",
      "-0.7410 -0.6619  0.5806  1.0560  2.1192 -0.4627 -0.1899 -0.7022 -0.1912  1.5956\n",
      "-0.1395 -0.0733 -0.2874  0.4459  1.2025  1.0226  0.4125  0.0802  1.1907  0.7434\n",
      " 0.4846 -1.3375  1.1207  0.6035  0.0398 -1.1000 -0.5878 -0.8736  0.6552  0.1844\n",
      "-1.5540 -0.0781 -0.7822 -0.7914 -1.4528 -0.1381  0.4843 -0.1314  0.3870  1.2114\n",
      "-0.1776  2.0843 -1.3551  1.1375 -0.5576 -0.4519 -0.1690 -1.6692  0.2887  0.3527\n",
      " 1.7354 -2.2682 -1.7332 -0.0454 -0.8921  1.3571  2.3527 -0.2226 -0.3170 -0.1601\n",
      "\n",
      "Columns 10 to 10 \n",
      " 0.0000\n",
      " 0.0000\n",
      " 0.0000\n",
      " 0.0000\n",
      " 0.0000\n",
      " 0.0000\n",
      " 0.0000\n",
      " 0.0000\n",
      " 0.0000\n",
      " 0.0000\n",
      " 0.0000\n",
      " 0.0000\n",
      " 0.0000\n",
      " 0.0000\n",
      " 0.0000\n",
      " 0.0000\n",
      " 0.0000\n",
      " 0.0000\n",
      " 0.0000\n",
      " 0.0000\n",
      "[torch.FloatTensor of size 20x11]\n",
      "\n",
      "\n",
      "\n",
      "Parameters of layer in the (w,b) format \n",
      "\n",
      "\n",
      "Columns 0 to 9 \n",
      "-0.2645  1.3447 -0.2172 -0.3744 -1.8756 -0.9588  0.4566 -0.2009  2.4943 -1.0138\n",
      " 1.3016 -2.4087 -0.4362  0.4558 -0.9106 -0.5391 -2.0016 -0.3311 -1.7777  0.9001\n",
      "-0.2602 -1.0382  0.6859  0.0199 -0.0205  0.9811  0.3549  1.0728  0.5096 -0.9810\n",
      "-0.6669 -0.3434 -1.0373 -2.2376 -1.1471 -0.8284  1.9912 -0.3734 -0.5917  1.2427\n",
      "-0.4702  0.6140 -0.1473  0.2608 -0.5781  0.3497  0.7667  0.2341  0.0997 -0.1106\n",
      " 1.1261  0.2547 -0.8312  0.6771 -1.5764  1.0641  1.3401  0.1483  0.0613 -0.7943\n",
      " 0.8891  0.1332 -0.2165  0.1755 -0.1552 -0.9926  0.0901 -0.7191  1.5844 -2.1285\n",
      "-0.6691  0.2988 -1.2371 -0.0090 -0.2648  1.0258  0.2548 -0.2212  0.0927  0.5774\n",
      " 1.9682  0.2578 -1.3754  0.4126 -0.8547  1.0212 -0.2187  0.2754 -0.6170  0.3567\n",
      "-0.4182 -0.4303  0.0546 -0.5816  0.4554  0.7481 -0.2392  0.1341  0.8608  0.0910\n",
      "\n",
      "Columns 10 to 19 \n",
      "-0.1662  0.7076  1.3697 -0.8818 -0.8408 -1.1547 -0.1362  1.1837  1.3412 -0.9902\n",
      " 1.6051  2.2002 -0.6195 -0.5698 -0.6710  3.2528 -0.9877 -2.0573 -0.0374  0.9614\n",
      "-0.3649 -0.1104  0.5173  1.1967  0.8009 -0.2977  0.0614  1.6702 -1.2523  0.3271\n",
      "-0.5524 -1.0886  0.0403  0.9756  0.6185 -0.0198 -0.0453 -0.4223  0.6917 -0.2217\n",
      " 0.5759  1.4322  0.5548  0.2214 -1.4806 -1.0451 -0.1002 -0.1019 -0.6501 -1.2677\n",
      " 0.8912 -0.4214  0.2648 -0.4719 -0.6391 -0.4995  0.5713 -1.1012 -0.3750 -0.9801\n",
      " 1.3403 -0.9762 -0.0421 -1.3982 -1.8685 -0.1973  0.0399  0.0169  0.3830  0.2416\n",
      "-1.1248 -0.0390 -0.2138 -2.1960 -1.0523 -1.6612 -0.8574 -0.1198 -1.2231  1.5972\n",
      " 2.7393 -1.2131  1.9662  1.3510 -0.8819  0.8508 -1.0824  1.0708  0.7702 -0.7781\n",
      " 0.3927  0.5126 -1.2093  1.2393 -0.1407 -1.6831  0.0442 -0.5910  1.2332 -1.7930\n",
      "\n",
      "Columns 20 to 20 \n",
      " 0.0000\n",
      " 0.0000\n",
      " 0.0000\n",
      " 0.0000\n",
      " 0.0000\n",
      " 0.0000\n",
      " 0.0000\n",
      " 0.0000\n",
      " 0.0000\n",
      " 0.0000\n",
      "[torch.FloatTensor of size 10x21]\n",
      "\n",
      "\n",
      "\n",
      "Parameters of layer in the (w,b) format \n",
      "\n",
      "\n",
      "Columns 0 to 9 \n",
      " 0.3850  1.3392 -0.9369 -0.6477  0.9225 -1.9010 -0.3436 -0.5472 -1.7558 -0.8533\n",
      " 0.1863  0.9015 -0.7113 -0.6915 -0.1948 -1.2930  0.0599  1.2744  1.2945 -0.2227\n",
      "\n",
      "Columns 10 to 10 \n",
      " 0.0000\n",
      " 0.0000\n",
      "[torch.FloatTensor of size 2x11]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print ('Parameters before training')\n",
    "for p in model.parameters():\n",
    "    print ('\\n\\nParameters of layer in the (w,b) format ')\n",
    "    print torch.cat((p.w,p.b),1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0\tLoss value is 253.253112845\n",
      "epoch: 50\tLoss value is 21.8429622894\n",
      "epoch: 100\tLoss value is 18.5221769223\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-d11a27937298>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_target\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-5-288ceccaf875>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, X, Y, eta, epoch)\u001b[0m\n\u001b[1;32m    232\u001b[0m                     \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mw\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0meta\u001b[0m\u001b[0;34m*\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdw\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 234\u001b[0;31m                 \u001b[0mL_mini\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpass_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_target\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnarrow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmini_batch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    235\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    236\u001b[0m             \u001b[0mL\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mL_mini\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-8d42138137db>\u001b[0m in \u001b[0;36mloss\u001b[0;34m(v, t)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0ml_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0ml_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mdloss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.train(train_input, train_target, lr, epoch);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameters after training\n",
      "\n",
      "\n",
      "Parameters of layer in the (w,b) format \n",
      "\n",
      "-0.4135 -2.2940  0.0000\n",
      "-0.5826 -0.2140  0.0000\n",
      "-1.4465 -0.6499  0.0000\n",
      "-0.8583 -0.3719  0.0000\n",
      "-0.9542  2.8501 -1.0068\n",
      "-0.8026 -0.9842  0.0000\n",
      "-0.2947  0.2961  0.2601\n",
      "-1.2274 -0.5868  0.0000\n",
      "-2.2710 -1.1534  2.6055\n",
      " 1.0697  0.5333 -0.4023\n",
      "[torch.FloatTensor of size 10x3]\n",
      "\n",
      "\n",
      "\n",
      "Parameters of layer in the (w,b) format \n",
      "\n",
      "\n",
      "Columns 0 to 9 \n",
      "-0.8951  1.3926  1.0456  0.6050 -0.3287  0.4710 -1.0475 -1.0665 -1.4633  0.4892\n",
      " 0.8866 -1.1448 -0.1763 -1.8596  0.0019  0.8653  0.6063  0.1321 -0.6618  1.2558\n",
      " 0.1574 -0.5391 -0.9920  0.3360  2.0496  0.6468  0.9083  1.2540  0.9091 -0.8074\n",
      "-1.3645  0.7671  0.8102  0.6664 -1.2113  1.0090 -0.1397 -0.9454 -2.8398  0.3265\n",
      " 1.0101  1.3345  0.6570 -0.0185 -0.3148  0.7455  0.0922  0.4798 -0.3699 -1.2471\n",
      " 1.6591  1.9904 -1.2563 -0.4836 -0.1899 -0.2568 -0.9376  0.4144 -0.4835 -1.2568\n",
      " 0.7089  2.1417  0.4376  0.5361  0.3810  0.1793  0.4315  0.4433 -0.0579 -1.0160\n",
      " 0.1610 -0.4964  1.3073  0.6303  0.1505 -0.0018 -1.6963  0.6965  0.3816 -0.3243\n",
      "-1.0133 -0.4030 -1.1582  0.8197 -0.4073  0.3127 -0.1843 -0.3187  0.5714 -0.3374\n",
      " 0.0347 -1.0142 -1.5014  0.8920  0.4406 -0.5783 -0.0879  1.3696 -0.2561  0.1609\n",
      " 3.0978  0.8311  1.8206 -1.9508  1.4423 -2.1428  0.4499  2.2078 -1.6616  0.2280\n",
      " 0.1455  0.5357  0.8443 -0.6890 -1.8175  0.7897  0.2382  1.8250  2.2056 -0.3475\n",
      " 0.4742 -1.5897  0.1263  0.5880  1.1948 -0.1132 -0.0729 -1.2987 -1.7783  0.4627\n",
      " 0.5288  0.9403 -0.8324 -0.2340 -0.0119  1.1727  0.6918  0.3777  0.3022  0.2299\n",
      "-0.7410 -0.6619  0.5806  1.0560  2.0708 -0.4627 -0.1469 -0.7022 -0.2576  1.6336\n",
      "-0.1395 -0.0733 -0.2874  0.4459  1.2483  1.0226  0.3400  0.0802  1.1407  0.8037\n",
      " 0.4846 -1.3375  1.1207  0.6035 -0.9046 -1.1000 -0.6982 -0.8736  1.0322  0.0623\n",
      "-1.5540 -0.0781 -0.7822 -0.7914 -1.2276 -0.1381  0.5479 -0.1314  0.1366  1.2422\n",
      "-0.1776  2.0843 -1.3551  1.1375 -0.6787 -0.4519 -0.2622 -1.6692  0.2372  0.3647\n",
      " 1.7354 -2.2682 -1.7332 -0.0454 -0.9195  1.3571  2.3549 -0.2226 -0.1169 -0.2665\n",
      "\n",
      "Columns 10 to 10 \n",
      "-0.0475\n",
      "-0.2849\n",
      "-0.4530\n",
      " 0.1875\n",
      "-0.0745\n",
      "-0.2522\n",
      "-0.0136\n",
      "-0.1041\n",
      " 0.2430\n",
      "-0.0327\n",
      "-0.6470\n",
      " 0.1978\n",
      "-0.3952\n",
      " 0.4696\n",
      " 0.0135\n",
      " 0.0860\n",
      " 0.3281\n",
      "-0.2585\n",
      " 0.0447\n",
      "-0.0338\n",
      "[torch.FloatTensor of size 20x11]\n",
      "\n",
      "\n",
      "\n",
      "Parameters of layer in the (w,b) format \n",
      "\n",
      "\n",
      "Columns 0 to 9 \n",
      "-0.3444  1.3942 -0.0215 -0.5192 -1.9942 -1.1133  0.4343 -0.2803  2.4669 -0.9925\n",
      " 1.2703 -2.4225 -0.3280  0.3313 -0.9558 -0.5990 -1.9789 -0.3090 -1.7834  0.9253\n",
      "-0.3430 -0.7367  0.8226 -0.0700 -0.1662  0.8228  0.2891  0.8371  0.3624 -0.9497\n",
      "-0.7012 -0.3133 -0.9932 -2.2598 -1.1594 -0.8560  1.9938 -0.4139 -0.6063  1.2368\n",
      "-0.4187  0.6000 -0.1015  0.2571 -0.5347  0.3605  0.7530  0.2467  0.1158 -0.1050\n",
      " 1.0820  0.2788 -0.8163  0.6685 -1.5818  1.0647  1.3787  0.1262  0.0380 -0.8065\n",
      " 0.8961  0.1002 -0.3097  0.2361 -0.1285 -0.9467  0.1360 -0.6890  1.5910 -2.1524\n",
      "-0.7983  0.3661 -1.1920 -0.0862 -0.3729  0.9411  0.4423 -0.3251 -0.0093  0.5664\n",
      " 2.1538  0.3464 -1.6982  0.9553 -1.0691  0.8652  0.1264  0.1042 -0.9291  0.5556\n",
      "-0.4160 -0.2943  0.0186 -0.5949  0.3986  0.7264 -0.3174  0.0562  0.8073  0.1197\n",
      "\n",
      "Columns 10 to 19 \n",
      "-0.1215  0.6737  1.4378 -0.7884 -0.6687 -0.9805 -0.1474  1.1964  1.2847 -0.9588\n",
      " 1.6938  2.0706 -0.5557 -0.5669 -0.5735  3.3481 -1.0153 -2.1128 -0.0957  0.8651\n",
      "-0.2194 -0.0773  0.6506  1.2366  0.9527 -0.1604 -0.0533  1.7203 -1.3089  0.4902\n",
      "-0.5049 -1.0897  0.0585  0.9783  0.6076  0.0111 -0.0733 -0.4193  0.6800 -0.1609\n",
      " 0.6208  1.4477  0.6092  0.1891 -1.4293 -1.0307 -0.0949 -0.0590 -0.6575 -1.2800\n",
      " 0.9203 -0.3898  0.2552 -0.4868 -0.6620 -0.5112  0.5398 -1.1722 -0.4038 -0.9408\n",
      " 1.3056 -0.9821 -0.0791 -1.4619 -1.9258 -0.2627  0.0497 -0.0488  0.3735  0.2529\n",
      "-0.9216 -0.4235 -0.1217 -2.3596 -1.0012 -1.5917 -1.0418 -0.4496 -1.4401  1.6886\n",
      " 3.5651 -2.4190  2.6680  1.1782 -0.6962  0.8870 -1.6835  0.5391  0.3476 -0.8976\n",
      " 0.3994  0.4721 -1.1726  1.2675 -0.1359 -1.6842  0.0079 -0.4875  1.2628 -1.7590\n",
      "\n",
      "Columns 20 to 20 \n",
      " 0.1748\n",
      " 0.0853\n",
      " 0.1351\n",
      " 0.0264\n",
      " 0.0158\n",
      "-0.0209\n",
      "-0.0605\n",
      " 0.0494\n",
      " 0.1084\n",
      " 0.0054\n",
      "[torch.FloatTensor of size 10x21]\n",
      "\n",
      "\n",
      "\n",
      "Parameters of layer in the (w,b) format \n",
      "\n",
      "\n",
      "Columns 0 to 9 \n",
      " 0.4031  1.6251 -0.5397 -0.3347  0.9002 -2.0770 -0.4717 -0.7661 -2.1192 -0.5355\n",
      "-0.1892  0.5226 -0.5739 -0.8530 -0.3100 -0.9645  0.1470  1.6639  2.2282  0.3051\n",
      "\n",
      "Columns 10 to 10 \n",
      " 0.2912\n",
      "-0.2052\n",
      "[torch.FloatTensor of size 2x11]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print ('Parameters after training')\n",
    "for p in model.parameters():\n",
    "    print ('\\n\\nParameters of layer in the (w,b) format ')\n",
    "    print torch.cat((p.w,p.b),1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error is 0.9%\n"
     ]
    }
   ],
   "source": [
    "# Testing the model\n",
    "Prediction = model.forward(test_input)[0]\n",
    "error(Prediction, test_target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot of the misclassifications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fe832a2c750>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "for idx,point in enumerate(test_input):\n",
    "    Col = 'b'* ( test_target[idx].dot(Prediction[idx]) > 0) + 'r' *( test_target[idx].dot(Prediction[idx]) <= 0)\n",
    "    plt.scatter(point[0],point[1], color = Col)\n",
    "plt.xlim(0,1)\n",
    "plt.ylim(0,1)\n",
    "plt.axis('equal')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Pytorch packages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because of wierd reasons, if you run the same model (we call it $\\texttt{pyModel}$) of the network, pytorch interprete some stuff as Variable and you can't run the our own-made model. just shutdown the notebook and start again!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "from random import randint\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from torch import Tensor\n",
    "from torch.nn import functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#seq = [Linear(2,10),'sig',Linear(10,20),'relu',Linear(20,10),'relu',Linear(10,2),'tanh']     \n",
    "pyModel = nn.Sequential(nn.Linear(2,10),nn.Sigmoid(),nn.Linear(10,20),nn.ReLU(),nn.Linear(20,10),nn.ReLU(),nn.Linear(10,2),nn.Tanh())\n",
    "\n",
    "train_input_py = Variable(train_input)\n",
    "train_target_py= Variable(train_target)\n",
    "test_input_py  = Variable(test_input)\n",
    "test_target_py = Variable(test_target)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-37-5275e2656a4f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0me\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpyModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_input_py\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_target_py\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0mpyModel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "### train the model\n",
    "epoch = 10000\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.SGD(pyModel.parameters(),lr = 0.3)\n",
    "for e in range(epoch):\n",
    "    output = pyModel(train_input_py)\n",
    "    loss = criterion(output,train_target_py)\n",
    "    pyModel.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing the model\n",
    "Prediction = pyModel(test_input_py)\n",
    "error(Prediction.data, test_target_py.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx,point in enumerate(test_input):\n",
    "    Col = 'b'* ( test_target[idx].dot(Prediction.data[idx]) > 0) + 'r' *( test_target[idx].dot(Prediction.data[idx]) <= 0)\n",
    "    plt.scatter(point[0],point[1], color = Col)\n",
    "\n",
    "plt.xlim(0,1)\n",
    "plt.ylim(0,1)\n",
    "plt.axis('equal')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final notes:\n",
    "      .           .            .        .     .             .             ...    ..."
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
