{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import Tensor\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Auxiliary Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear:\n",
    "    \"\"\"\n",
    "    An class that contains objects which only store layar's in/out connections dimension\n",
    "    \n",
    "    input_s:\n",
    "        dim_in      :  the input_ dimension of fully connected layer\n",
    "        dim_out     :  the output_ dimension of fully connected layer\n",
    "        \n",
    "    returns:\n",
    "        Nothing \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self,dim_in, dim_out):\n",
    "        self.input_ = dim_in\n",
    "        self.output_ = dim_out          \n",
    "    # TODO: The linear is really wierd thing... all we get here is already in the upper class. we may omit this somehow\n",
    "\n",
    "#---------------------------------------------------------------------------------------------\n",
    "        \n",
    "def Activation(code,input_):\n",
    "    \"\"\"\n",
    "    A class that specify the needed activation with respect to the following code conversion\n",
    "        0: Relu(x)\n",
    "        1: Tanh(x)\n",
    "        2: Sigmoid(x)\n",
    "    \n",
    "    This class works as functional package of pytorch\n",
    "    \n",
    "    input_s:\n",
    "        code        :  the code for each activation (0,1,2)\n",
    "        input__tensor:  the input_ tensor\n",
    "        \n",
    "    returns:\n",
    "        result      :  the output_ of requested activation function with the same shape as input_ tensor\n",
    "    \"\"\"\n",
    "    result = Tensor(input_.shape)\n",
    "    # Relu\n",
    "    if code ==0:\n",
    "        result = input_ - (input_<0).float()*input_\n",
    "    # Tanh\n",
    "    elif code ==1:\n",
    "        result = torch.tanh(input_)\n",
    "    # Sig\n",
    "    elif code ==2:\n",
    "        result = 1.0/(1 + torch.exp(-input_))\n",
    "    # linear\n",
    "    elif code ==3:\n",
    "        result = input_\n",
    "    # error\n",
    "    else: raise ValueError('Unknown Code For Activation')\n",
    "        \n",
    "    return result \n",
    "\n",
    "#---------------------------------------------------------------------------------------------\n",
    "\n",
    "def dActivation(code,input_):\n",
    "    \"\"\"\n",
    "    A class that specify the needed derivative of activation with the same encoding convenstion\n",
    "        0: dRelu(x)\n",
    "        1: dTanh(x)\n",
    "        2: dSigmoid(x)\n",
    "    \n",
    "    This class works as functional package of pytorch\n",
    "    \n",
    "    input_s:\n",
    "        code        :  the code for each activation (0,1,2)\n",
    "        input__tensor:  the input_ tensor\n",
    "        \n",
    "    returns:\n",
    "        result      :  the output_ of requested activation function with the same shape as input_ tensor\n",
    "    \"\"\"\n",
    "    result = Tensor.new(input_)\n",
    "    # dRelu\n",
    "    if code ==0:\n",
    "        result = Tensor(input_.shape).fill_(1.0) - (input_<=0).float()*Tensor(input_.shape).fill_(1.0)\n",
    "    # dTanh\n",
    "    elif code ==1:\n",
    "        result = 1-(torch.tanh(input_))**2\n",
    "    # dSig\n",
    "    elif code ==2:\n",
    "        result = Activation(code,input_)*(1-Activation(code,input_))\n",
    "    # linear\n",
    "    elif code ==3:\n",
    "        result = Tensor(input_.shape).fill_(1.0)\n",
    "    else: raise ValueError('Unknown Code For derivative of Activation')\n",
    "    \n",
    "    return result \n",
    "\n",
    "#---------------------------------------------------------------------------------------------\n",
    "    \n",
    "class PassPar:    # change the name to: \"ForwardAns\n",
    "    \"\"\"\n",
    "    This class keeps track of all the variables produced in forward pass of some layer. i.e, x and s.\n",
    "    \n",
    "    inputs:\n",
    "        N           :  number of data\n",
    "        dim_in      :  the input_ dimension of fully connected layer\n",
    "        dim_out     :  the output_ dimension of fully connected layer\n",
    "        \n",
    "    returns:\n",
    "        Nothing\n",
    "    \"\"\"\n",
    "    def __init__(self,dim_in, dim_out, N):\n",
    "        self.s = Tensor(N,dim_out).fill_(0)           # s after each layer\n",
    "        self.x = Tensor(N,dim_out).fill_(0)           # x after each layer:   x = Activation (s)\n",
    "        self.db = Tensor(N, dim_out,1).fill_(0)       # dL/db for each input\n",
    "        self.dw = Tensor(N, dim_out,dim_in).fill_(0)  # dL/dw for each input\n",
    "        self.ds = Tensor(N, dim_out).fill_(0)         # dL/ds for each input\n",
    "        self.dx = Tensor(N, dim_out).fill_(0)         # dL/dx for each input\n",
    "        \n",
    "#---------------------------------------------------------------------------------------------\n",
    "\n",
    "class Param:\n",
    "    \"\"\"\n",
    "    This class contains parameters of each layer. We initialize them in constructor.\n",
    "    \n",
    "    inputs:\n",
    "        dim_in      :  the input dimension of fully connected layer\n",
    "        dim_out     :  the output dimension of fully connected layer\n",
    "        b           :  bias vector\n",
    "        w           :  weight matrix\n",
    "        db          :  grad wrt bias\n",
    "        dw          :  grad wrt weight matrix\n",
    "    modules:\n",
    "        data        :  returns the parameters as one tensor \n",
    "        grad        :  returns the grad of Loss wrt to parameter as one tensor\n",
    "    \"\"\"\n",
    "    def __init__(self, dim_in, dim_out):\n",
    "        self.dim_in  = dim_in\n",
    "        self.dim_out = dim_out\n",
    "        self.b = Tensor(dim_out,1).fill_(0)          # bias of each layer\n",
    "        self.w = Tensor(dim_out,dim_in).normal_()    # w of each layer\n",
    "        self.db = Tensor(dim_out,1).fill_(0)         # dL/dbias of each layer\n",
    "        self.dw = Tensor(dim_out,dim_in).fill_(0)    # dL/dw of each layer\n",
    "    \n",
    "    \n",
    "    def data(self):\n",
    "        return (torch.cat((self.w, self.b),1))\n",
    "    \n",
    "    def grad(self):\n",
    "        \"\"\"        print (self.dw.shape)\n",
    "        print (self.db.shape)\n",
    "        print (self.w.shape)\n",
    "        print (self.b.shape)\"\"\"\n",
    "        return (torch.cat((self.dw, self.db),1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LossMSE and Error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(v, t):\n",
    "    l_ = torch.sum(torch.pow(v-t,2))/(len(v))\n",
    "    return l_\n",
    "\n",
    "def dloss(v,t):\n",
    "    return 2.*(v-t)/(len(v))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def error(Prediction, target_):\n",
    "    # if the score of two classes are equal, we consider it as a misclassification\n",
    "    e = 0\n",
    "    for n in list(range(len(target_))):\n",
    "        if target_[n,0]>target_[n,1]:                  # class 0\n",
    "            if Prediction[n,0]<=Prediction[n,1]: e +=1\n",
    "        else:                                      # class 1\n",
    "            if Prediction[n,0]>=Prediction[n,1]: e +=1\n",
    "    print ('error is {}%'.format(1./len(target_)*e*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Network Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net:\n",
    "    \"\"\"\n",
    "    The network class. It has the following methods:\n",
    "        param      :  returns the parameter which is asked for. Not the data! The object... \n",
    "                        Data is accessible through object.data method)\n",
    "        make_arch  :  makes the architecture of the network by taking a sequential list of [fc1,act1,fc2,act2,...]\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, seq, X,Y,mini_batch_size=None):\n",
    "        \n",
    "        self.n_layer =len(seq)//2                           # number of layer  ((*/2) is because of the activations...)\n",
    "        self.N = len(X)                                     # nb of batch\n",
    "        if mini_batch_size!=None: \n",
    "            self.N = mini_batch_size\n",
    "            \n",
    "        self.param_list    = list(range(self.n_layer))      # Stores parameters of each layer (W,b) and their grads  \n",
    "        self.pass_list     = list(range(self.n_layer+1))    # Stores parameters evaluted during forward/backward pass\n",
    "        \n",
    "        self.act_list = list(range(self.n_layer))           # stores the requested activation functions in codes. Elements are \"0\",\"1\" or \"2\"\n",
    "        self.make_arch(seq)                                 # makes the architecture based on the the list \"seq\"\n",
    "        \n",
    "\n",
    "    def make_arch(self,seq):    # makeArch\n",
    "        \"\"\"\n",
    "        This function fills param_list and act_list and also, evaluates number of layers.\n",
    "        \n",
    "        input:\n",
    "            seq : a list that contains both activation and layers in a sequential manner\n",
    "                  Example: [Linear(3,5), 'relu', Linear(5,64), 'Sig', Linear(64,1), 'Tanh']\n",
    "        \"\"\"\n",
    "        \n",
    "        seq_len = len(seq)                  # number of layer *2 (because of the activations...)\n",
    "        for layer in list(range(0,seq_len,2)):\n",
    "             \n",
    "            # seq[layer] is an instance of object \"Linear\". Here we get the in/out dim of the layer\n",
    "            dim_in, dim_out = seq[layer].input_ , seq[layer].output_ \n",
    "            \n",
    "            # making a new Param instance and adding it to the param_list\n",
    "            self.param_list[int(layer/2)]= Param(dim_in, dim_out) \n",
    "            \n",
    "            # activation recognition : encode activations in \"act_list\"\n",
    "            if seq[layer+1]=='relu':\n",
    "                self.act_list.append(0)\n",
    "            elif seq[layer+1]=='tanh':\n",
    "                self.act_list.append(1)\n",
    "            elif seq[layer+1]=='sig':\n",
    "                self.act_list.append(2)\n",
    "            elif seq[layer+1]=='lin':\n",
    "                self.act_list.append(3)\n",
    "            else: raise ValueError('Unknown Activation')\n",
    "                    \n",
    "    \n",
    "    def forward(self,X): \n",
    "        \"\"\"\n",
    "        This method evaluates the forward pass and returns the values. This function is written such that it take a\n",
    "        batch input and returns the forward pass of the batch.\n",
    "        \n",
    "        input: \n",
    "            X    :   a tensor of size(B, d_in) \n",
    "            \n",
    "        returns:\n",
    "            s    :   a tensor of size(B, d_out) \n",
    "            x    :   a tensor of size(B, d_out) \n",
    "        \"\"\"\n",
    "        \n",
    "        x=X\n",
    "        for layer, prm in enumerate(self.param_list):   # layer = [0,1,2,...,nb_layer -1]  ;  prm = param_list[layer]\n",
    "            s = (x.mm(prm.w.t()) + prm.b.t())           # written consistant for batch :  s = (Wx+b).t()  (N,d_out) \n",
    "            x = Activation(self.act_list[layer], s)     # size = (N,d_out)       \n",
    "        return x,s\n",
    "    \n",
    "        \"\"\"\n",
    "        Hint:\n",
    "            prm.w.shape = (d_out, d_in)\n",
    "            prm.b.shpae = (d_out, 1)\n",
    "            X.shape     = (N,d_in)\n",
    "            x.shape     = (N,d_out)\n",
    "            s.shape     = (N.d_out)\n",
    "        \"\"\"\n",
    "    \n",
    "    \n",
    "    def backward (self,X,Y):\n",
    "        \"\"\"\n",
    "        This method fills pass_list by constructing instances of PassPar.The object PassPar, contains\n",
    "        (s,x) and also the gradients with respect to all parameteres of each layer (dL/dw, dL/db, dL/ds, dL/dx)\n",
    "        during a froward and backwar pass.\n",
    "        \n",
    "        These gradients are responsible for all batch data. The sum of all parameters determines the\n",
    "        total gradient of the batch. This summation is stored in the respective attributes of param_list that \n",
    "        contains Param objects.\n",
    "        \n",
    "        This function return the values of MSE loss during each pass.\n",
    "        \n",
    "        input:\n",
    "            X        :   training set of size (B,d)\n",
    "            Y        :   target set of size (B,d)\n",
    "            \n",
    "        returns:\n",
    "            loss     :   value of MSE loss\n",
    "        \"\"\"\n",
    "        \n",
    "        # add the input X to the pass list\n",
    "        self.pass_list[0]= PassPar(self.param_list[0].dim_in, self.param_list[0].dim_in, self.N)    # Note that dim = dim_in for inputs\n",
    "        self.pass_list[0].x =X    # x0\n",
    "        self.pass_list[0].s =X    # s0 is set to be x0\n",
    "        \n",
    "        \n",
    "        # this computes forward pass and saves s and x\n",
    "        x=X\n",
    "        for layer, prm in enumerate(self.param_list): # layer = [0,1,2,...,nb_layer -1]  ; prm = param_list[layer]  \n",
    "            \n",
    "            \"\"\"\n",
    "            hint: PassPar has following attributes:\n",
    "                s  = Tensor(N, dim_out)           \n",
    "                x  = Tensot(N, dim_out)\n",
    "            \"\"\"\n",
    "            \n",
    "            self.pass_list[layer+1]= PassPar(prm.dim_in, prm.dim_out, self.N)  # instantiating pass parameters\n",
    "            \n",
    "            #print('hey there and shape of pass_list[layer+1].dw is {}'.format(self.pass_list[layer+1].dw.shape))\n",
    "            \n",
    "            s = (x.mm(prm.w.t()) + prm.b.t())         # consistant with batch. s.shape = (N,d_out)\n",
    "            self.pass_list[layer+1].s = s             \n",
    "            x = Activation(self.act_list[layer], s)    \n",
    "            self.pass_list[layer+1].x = x\n",
    "         \n",
    "        \n",
    "        # this computes backward\n",
    "        for layer in list(range (self.n_layer,-1,-1)): # layer=[nb_layer, nb_layer-1 , ..., 1]\n",
    "            \n",
    "            \"\"\"\n",
    "            hint: PassPar also has the following attributes:\n",
    "                db = Tensor(N, dim_out,1)         # dL/db\n",
    "                dw = Tensor(N, dim_out,dim_in)    # dL/dw\n",
    "                ds = Tensor(N, dim_out)           # dL/ds\n",
    "                dx = Tensor(N, dim_out)           # dL/dx\n",
    "            \"\"\"\n",
    "            \n",
    "            # dl/dx : size = N,d_out\n",
    "            if layer == self.n_layer:\n",
    "                self.pass_list[layer].dx = dloss(x,Y)   \n",
    "            else:\n",
    "                self.pass_list[layer].dx = self.pass_list[layer+1].ds.mm(self.param_list[layer].w)\n",
    "            \"\"\"\n",
    "            hint:\n",
    "                dloss(x,Y)                 = (N, d_out_{last_layer})\n",
    "                pass_list[layer+1].ds      = (N, d_out_{layer+1})\n",
    "                param_list[layer].w        = (d_out_{layer+1}, d_in_{layer+1}) = (d_out_{layer+1}, d_out_{layer}) \n",
    "            \"\"\"\n",
    "                \n",
    "                \n",
    "            #dl/ds : size = N,d_out\n",
    "            self.pass_list[layer].ds = self.pass_list[layer].dx * dActivation(self.act_list[layer-1], self.pass_list[layer].s ) \n",
    "            \"\"\"\n",
    "            hint:\n",
    "                self.act_list[layer-1] :  activation type of layer = layer (0,1,2 or 3)\n",
    "                pass_list[layer].dx    =  (N, d_out)\n",
    "                pass_list[layer].s     =  (N, d_out)\n",
    "                pass_list[layer].ds    =  (N, d_out)\n",
    "                dActivation(code, s)   =  (N, d_out)\n",
    "            \"\"\"\n",
    "            \n",
    "            \n",
    "            #dl/dw : size = (N, d_out, d_in)\n",
    "            ds_unsq = self.pass_list[layer].ds.unsqueeze(1)    \n",
    "            x_unsq  = self.pass_list[layer-1].x.unsqueeze(2)\n",
    "            self.pass_list[layer].dw =  (x_unsq * ds_unsq).transpose(1,2)         \n",
    "            \"\"\"\n",
    "            hint:\n",
    "                pass_list[layer].ds        = (N, d_out)\n",
    "                self.pass_list[layer-1].x  = (N, d_in)\n",
    "                ds_unsq                    = (N, 1, d_out)\n",
    "                dx_unsq                    = (N, d_in, 1)\n",
    "                pass_list[layer].dw        = (N, d_out, d_in)\n",
    "            \"\"\"\n",
    "            \n",
    "            \n",
    "            #dl/db : size = (N, d_out)\n",
    "            self.pass_list[layer].db = self.pass_list[layer].ds\n",
    "            \n",
    "            \"\"\"\n",
    "            hint:\n",
    "                pass_list[layer].ds        = (N, d_out)\n",
    "                pass_list[layer].db        = (N, d_out)\n",
    "            \"\"\"\n",
    "\n",
    "        # summing all batch grads\n",
    "        for layer in list (range(0,self.n_layer)):\n",
    "            self.param_list[layer].db = self.pass_list[layer+1].db.sum(0).unsqueeze(1)  \n",
    "            self.param_list[layer].dw = self.pass_list[layer+1].dw.sum(0)\n",
    "            #print('hey there in sum and shape of pass_list[layer+1].dw is {}'.format(self.pass_list[layer+1].dw.shape))\n",
    "    \n",
    "    \n",
    "    def parameters (self):\n",
    "        return self.param_list\n",
    "\n",
    "    \"\"\"    \n",
    "    def train_thresh(self,X,Y,eta,thresh):\n",
    "        L0=0\n",
    "        counter =0\n",
    "        while counter<10000:\n",
    "            self.backward(X,Y)\n",
    "            L= loss()\n",
    "            if (abs(L-L0)<thresh):\n",
    "                break\n",
    "            for p in self.parameters():\n",
    "                p.b -= eta* p.db\n",
    "                p.w -= eta* p.dw\n",
    "            \n",
    "            L0=L\n",
    "            if counter%50==0: print('counter: {}\\tLoss value is {}'.format(counter, L))\n",
    "            counter += 1\n",
    "    \"\"\"        \n",
    "    def train(self, X, Y, eta, epoch):\n",
    "        \n",
    "        mini_batch_size = self.N   # N if no batch size is provided at initialization of network, else it is already mini_batch_size\n",
    "        e  = 0\n",
    "        while e< epoch:\n",
    "            L_mini = 0\n",
    "            for batch in list(range (0, train_input.size(0), mini_batch_size)):\n",
    "                \n",
    "                # Note that here we feed the backward by mini_train and mini_target defined as below: \n",
    "                #\n",
    "                #     mini_train = train_input.narrow(0, b, mini_batch_size)\n",
    "                #     mini_target= train_target.narrow(0, b, mini_batch_size)\n",
    "                \n",
    "                self.backward(train_input.narrow(0, batch, mini_batch_size) ,\n",
    "                              train_target.narrow(0, batch, mini_batch_size) )     \n",
    "                \n",
    "                for p in self.parameters():\n",
    "                    p.b -= eta* p.db\n",
    "                    p.w -= eta* p.dw\n",
    "                \n",
    "                L_mini += loss(self.pass_list[-1].x, train_target.narrow(0, batch, mini_batch_size))\n",
    "                    \n",
    "            L=L_mini\n",
    "            if e%50==0: print('epoch: {}\\tLoss value is {}'.format(e, L))\n",
    "            e+=1\n",
    "     \n",
    "    \"\"\"\n",
    "    We don't need to use this method. Since all the gradients are not accumelated in our code.\n",
    "            \n",
    "    def zero_grad(self):\n",
    "    \n",
    "        for g in self.grad_list:\n",
    "            g.db.fill_(0)\n",
    "            g.dw.fill_(0)\n",
    "            g.dx.fill_(0)\n",
    "            g.ds.fill_(0)\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating a training and a test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train set\n",
    "\n",
    "N_train=10000             # number of points\n",
    "x=torch.rand(N_train)    # x pos\n",
    "y=torch.rand(N_train)    # y pos\n",
    "\n",
    "train_input = torch.cat((x.unsqueeze_(1),y.unsqueeze_(1)),1)       # input batch\n",
    "train_target =Tensor(N_train,2)              # target batch\n",
    "\n",
    "#r=1/(2*np.pi) #r^2\n",
    "r=1\n",
    "\n",
    "# making a hot vector target\n",
    "train_target[:,0] = (x**2 + y**2 <=r).float() - ( x**2 + y**2 > r).float()   # 1 if inside circle, else -1\n",
    "train_target[:,1] = (x**2 + y**2 > r).float() - ( x**2 + y**2 <=r).float()   # -1 if inside circle else 1\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Test set\n",
    "\n",
    "N_test=1000              # number of points\n",
    "x=torch.rand(N_test)    # x pos\n",
    "y=torch.rand(N_test)    # y pos\n",
    "\n",
    "test_input = torch.cat((x.unsqueeze_(1),y.unsqueeze_(1)),1)       # input batch\n",
    "test_target =Tensor(N_test,2)              # target batch\n",
    "\n",
    "#r=1/(2*np.pi) #r^2\n",
    "r=1\n",
    "\n",
    "# making a hot vector target\n",
    "test_target[:,0] = (x**2 + y**2 <=r).float() - ( x**2 + y**2 > r).float()   # 1 if inside circle, else -1\n",
    "test_target[:,1] = (x**2 + y**2 > r).float() - ( x**2 + y**2 <=r).float()   # -1 if inside circle else 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.01\n",
    "epoch = 1000\n",
    "batch_size = 50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training a simple network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq = [Linear(2,10),'sig',Linear(10,20),'relu',Linear(20,10),'relu',Linear(10,2),'tanh']\n",
    "model = Net(seq, train_input, train_target, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameters before training\n",
      "\n",
      "\n",
      "Parameters of layer in the (w,b) format \n",
      "\n",
      "-0.4135 -2.2940  0.0000\n",
      "-0.5826 -0.2140  0.0000\n",
      "-1.4465 -0.6499  0.0000\n",
      "-0.8583 -0.3719  0.0000\n",
      "-0.2567  1.6391  0.0000\n",
      "-0.8026 -0.9842  0.0000\n",
      " 0.3631  0.3333  0.0000\n",
      "-1.2274 -0.5868  0.0000\n",
      "-0.7886  0.2846  0.0000\n",
      " 0.7370  0.0801  0.0000\n",
      "[torch.FloatTensor of size 10x3]\n",
      "\n",
      "\n",
      "\n",
      "Parameters of layer in the (w,b) format \n",
      "\n",
      "\n",
      "Columns 0 to 9 \n",
      "-0.8951  1.3926  1.0456  0.6050 -0.1991  0.4710 -0.8784 -1.0665 -0.9049  0.1054\n",
      " 0.8866 -1.1448 -0.1763 -1.8596 -0.0878  0.8653  0.6362  0.1321 -0.6461  1.4947\n",
      " 0.1574 -0.5391 -0.9920  0.3360  2.1402  0.6468  0.9763  1.2540  0.7675 -0.3045\n",
      "-1.3645  0.7671  0.8102  0.6664 -1.2551  1.0090 -0.1184 -0.9454 -2.7526  0.1086\n",
      " 1.0101  1.3345  0.6570 -0.0185  0.1281  0.7455  0.2341  0.4798 -0.2629 -1.2186\n",
      " 1.6591  1.9904 -1.2563 -0.4836 -0.0747 -0.2568 -0.8441  0.4144 -0.3495 -1.2126\n",
      " 0.7089  2.1417  0.4376  0.5361  0.3941  0.1793  0.3001  0.4433 -0.1051 -1.0083\n",
      " 0.1610 -0.4964  1.3073  0.6303  0.0880 -0.0018 -1.7361  0.6965  0.5794 -0.3885\n",
      "-1.0133 -0.4030 -1.1582  0.8197  0.1333  0.3127 -0.2597 -0.3187  0.5441 -0.5236\n",
      " 0.0347 -1.0142 -1.5014  0.8920 -0.0594 -0.5783 -0.1304  1.3696 -0.2035  0.1256\n",
      " 3.0978  0.8311  1.8206 -1.9508  0.5828 -2.1428  0.6132  2.2078 -0.9460  0.2180\n",
      " 0.1455  0.5357  0.8443 -0.6890 -0.4162  0.7897  0.6822  1.8250  1.8868  0.0819\n",
      " 0.4742 -1.5897  0.1263  0.5880 -0.0170 -0.1132 -0.2586 -1.2987 -1.2650  0.3208\n",
      " 0.5288  0.9403 -0.8324 -0.2340 -0.1803  1.1727  0.5259  0.3777  0.4373 -0.2178\n",
      "-0.7410 -0.6619  0.5806  1.0560  2.1192 -0.4627 -0.1899 -0.7022 -0.1912  1.5956\n",
      "-0.1395 -0.0733 -0.2874  0.4459  1.2025  1.0226  0.4125  0.0802  1.1907  0.7434\n",
      " 0.4846 -1.3375  1.1207  0.6035  0.0398 -1.1000 -0.5878 -0.8736  0.6552  0.1844\n",
      "-1.5540 -0.0781 -0.7822 -0.7914 -1.4528 -0.1381  0.4843 -0.1314  0.3870  1.2114\n",
      "-0.1776  2.0843 -1.3551  1.1375 -0.5576 -0.4519 -0.1690 -1.6692  0.2887  0.3527\n",
      " 1.7354 -2.2682 -1.7332 -0.0454 -0.8921  1.3571  2.3527 -0.2226 -0.3170 -0.1601\n",
      "\n",
      "Columns 10 to 10 \n",
      " 0.0000\n",
      " 0.0000\n",
      " 0.0000\n",
      " 0.0000\n",
      " 0.0000\n",
      " 0.0000\n",
      " 0.0000\n",
      " 0.0000\n",
      " 0.0000\n",
      " 0.0000\n",
      " 0.0000\n",
      " 0.0000\n",
      " 0.0000\n",
      " 0.0000\n",
      " 0.0000\n",
      " 0.0000\n",
      " 0.0000\n",
      " 0.0000\n",
      " 0.0000\n",
      " 0.0000\n",
      "[torch.FloatTensor of size 20x11]\n",
      "\n",
      "\n",
      "\n",
      "Parameters of layer in the (w,b) format \n",
      "\n",
      "\n",
      "Columns 0 to 9 \n",
      "-0.2645  1.3447 -0.2172 -0.3744 -1.8756 -0.9588  0.4566 -0.2009  2.4943 -1.0138\n",
      " 1.3016 -2.4087 -0.4362  0.4558 -0.9106 -0.5391 -2.0016 -0.3311 -1.7777  0.9001\n",
      "-0.2602 -1.0382  0.6859  0.0199 -0.0205  0.9811  0.3549  1.0728  0.5096 -0.9810\n",
      "-0.6669 -0.3434 -1.0373 -2.2376 -1.1471 -0.8284  1.9912 -0.3734 -0.5917  1.2427\n",
      "-0.4702  0.6140 -0.1473  0.2608 -0.5781  0.3497  0.7667  0.2341  0.0997 -0.1106\n",
      " 1.1261  0.2547 -0.8312  0.6771 -1.5764  1.0641  1.3401  0.1483  0.0613 -0.7943\n",
      " 0.8891  0.1332 -0.2165  0.1755 -0.1552 -0.9926  0.0901 -0.7191  1.5844 -2.1285\n",
      "-0.6691  0.2988 -1.2371 -0.0090 -0.2648  1.0258  0.2548 -0.2212  0.0927  0.5774\n",
      " 1.9682  0.2578 -1.3754  0.4126 -0.8547  1.0212 -0.2187  0.2754 -0.6170  0.3567\n",
      "-0.4182 -0.4303  0.0546 -0.5816  0.4554  0.7481 -0.2392  0.1341  0.8608  0.0910\n",
      "\n",
      "Columns 10 to 19 \n",
      "-0.1662  0.7076  1.3697 -0.8818 -0.8408 -1.1547 -0.1362  1.1837  1.3412 -0.9902\n",
      " 1.6051  2.2002 -0.6195 -0.5698 -0.6710  3.2528 -0.9877 -2.0573 -0.0374  0.9614\n",
      "-0.3649 -0.1104  0.5173  1.1967  0.8009 -0.2977  0.0614  1.6702 -1.2523  0.3271\n",
      "-0.5524 -1.0886  0.0403  0.9756  0.6185 -0.0198 -0.0453 -0.4223  0.6917 -0.2217\n",
      " 0.5759  1.4322  0.5548  0.2214 -1.4806 -1.0451 -0.1002 -0.1019 -0.6501 -1.2677\n",
      " 0.8912 -0.4214  0.2648 -0.4719 -0.6391 -0.4995  0.5713 -1.1012 -0.3750 -0.9801\n",
      " 1.3403 -0.9762 -0.0421 -1.3982 -1.8685 -0.1973  0.0399  0.0169  0.3830  0.2416\n",
      "-1.1248 -0.0390 -0.2138 -2.1960 -1.0523 -1.6612 -0.8574 -0.1198 -1.2231  1.5972\n",
      " 2.7393 -1.2131  1.9662  1.3510 -0.8819  0.8508 -1.0824  1.0708  0.7702 -0.7781\n",
      " 0.3927  0.5126 -1.2093  1.2393 -0.1407 -1.6831  0.0442 -0.5910  1.2332 -1.7930\n",
      "\n",
      "Columns 20 to 20 \n",
      " 0.0000\n",
      " 0.0000\n",
      " 0.0000\n",
      " 0.0000\n",
      " 0.0000\n",
      " 0.0000\n",
      " 0.0000\n",
      " 0.0000\n",
      " 0.0000\n",
      " 0.0000\n",
      "[torch.FloatTensor of size 10x21]\n",
      "\n",
      "\n",
      "\n",
      "Parameters of layer in the (w,b) format \n",
      "\n",
      "\n",
      "Columns 0 to 9 \n",
      " 0.3850  1.3392 -0.9369 -0.6477  0.9225 -1.9010 -0.3436 -0.5472 -1.7558 -0.8533\n",
      " 0.1863  0.9015 -0.7113 -0.6915 -0.1948 -1.2930  0.0599  1.2744  1.2945 -0.2227\n",
      "\n",
      "Columns 10 to 10 \n",
      " 0.0000\n",
      " 0.0000\n",
      "[torch.FloatTensor of size 2x11]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print ('Parameters before training')\n",
    "for p in model.parameters():\n",
    "    print ('\\n\\nParameters of layer in the (w,b) format ')\n",
    "    print torch.cat((p.w,p.b),1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0\tLoss value is 253.253112845\n",
      "epoch: 50\tLoss value is 21.8429622894\n",
      "epoch: 100\tLoss value is 18.5221769223\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-d11a27937298>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_target\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-5-288ceccaf875>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, X, Y, eta, epoch)\u001b[0m\n\u001b[1;32m    232\u001b[0m                     \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mw\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0meta\u001b[0m\u001b[0;34m*\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdw\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 234\u001b[0;31m                 \u001b[0mL_mini\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpass_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_target\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnarrow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmini_batch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    235\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    236\u001b[0m             \u001b[0mL\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mL_mini\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-8d42138137db>\u001b[0m in \u001b[0;36mloss\u001b[0;34m(v, t)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0ml_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0ml_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mdloss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.train(train_input, train_target, lr, epoch);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameters after training\n",
      "\n",
      "\n",
      "Parameters of layer in the (w,b) format \n",
      "\n",
      "-0.4135 -2.2940  0.0000\n",
      "-0.5826 -0.2140  0.0000\n",
      "-1.4465 -0.6499  0.0000\n",
      "-0.8583 -0.3719  0.0000\n",
      "-0.9542  2.8501 -1.0068\n",
      "-0.8026 -0.9842  0.0000\n",
      "-0.2947  0.2961  0.2601\n",
      "-1.2274 -0.5868  0.0000\n",
      "-2.2710 -1.1534  2.6055\n",
      " 1.0697  0.5333 -0.4023\n",
      "[torch.FloatTensor of size 10x3]\n",
      "\n",
      "\n",
      "\n",
      "Parameters of layer in the (w,b) format \n",
      "\n",
      "\n",
      "Columns 0 to 9 \n",
      "-0.8951  1.3926  1.0456  0.6050 -0.3287  0.4710 -1.0475 -1.0665 -1.4633  0.4892\n",
      " 0.8866 -1.1448 -0.1763 -1.8596  0.0019  0.8653  0.6063  0.1321 -0.6618  1.2558\n",
      " 0.1574 -0.5391 -0.9920  0.3360  2.0496  0.6468  0.9083  1.2540  0.9091 -0.8074\n",
      "-1.3645  0.7671  0.8102  0.6664 -1.2113  1.0090 -0.1397 -0.9454 -2.8398  0.3265\n",
      " 1.0101  1.3345  0.6570 -0.0185 -0.3148  0.7455  0.0922  0.4798 -0.3699 -1.2471\n",
      " 1.6591  1.9904 -1.2563 -0.4836 -0.1899 -0.2568 -0.9376  0.4144 -0.4835 -1.2568\n",
      " 0.7089  2.1417  0.4376  0.5361  0.3810  0.1793  0.4315  0.4433 -0.0579 -1.0160\n",
      " 0.1610 -0.4964  1.3073  0.6303  0.1505 -0.0018 -1.6963  0.6965  0.3816 -0.3243\n",
      "-1.0133 -0.4030 -1.1582  0.8197 -0.4073  0.3127 -0.1843 -0.3187  0.5714 -0.3374\n",
      " 0.0347 -1.0142 -1.5014  0.8920  0.4406 -0.5783 -0.0879  1.3696 -0.2561  0.1609\n",
      " 3.0978  0.8311  1.8206 -1.9508  1.4423 -2.1428  0.4499  2.2078 -1.6616  0.2280\n",
      " 0.1455  0.5357  0.8443 -0.6890 -1.8175  0.7897  0.2382  1.8250  2.2056 -0.3475\n",
      " 0.4742 -1.5897  0.1263  0.5880  1.1948 -0.1132 -0.0729 -1.2987 -1.7783  0.4627\n",
      " 0.5288  0.9403 -0.8324 -0.2340 -0.0119  1.1727  0.6918  0.3777  0.3022  0.2299\n",
      "-0.7410 -0.6619  0.5806  1.0560  2.0708 -0.4627 -0.1469 -0.7022 -0.2576  1.6336\n",
      "-0.1395 -0.0733 -0.2874  0.4459  1.2483  1.0226  0.3400  0.0802  1.1407  0.8037\n",
      " 0.4846 -1.3375  1.1207  0.6035 -0.9046 -1.1000 -0.6982 -0.8736  1.0322  0.0623\n",
      "-1.5540 -0.0781 -0.7822 -0.7914 -1.2276 -0.1381  0.5479 -0.1314  0.1366  1.2422\n",
      "-0.1776  2.0843 -1.3551  1.1375 -0.6787 -0.4519 -0.2622 -1.6692  0.2372  0.3647\n",
      " 1.7354 -2.2682 -1.7332 -0.0454 -0.9195  1.3571  2.3549 -0.2226 -0.1169 -0.2665\n",
      "\n",
      "Columns 10 to 10 \n",
      "-0.0475\n",
      "-0.2849\n",
      "-0.4530\n",
      " 0.1875\n",
      "-0.0745\n",
      "-0.2522\n",
      "-0.0136\n",
      "-0.1041\n",
      " 0.2430\n",
      "-0.0327\n",
      "-0.6470\n",
      " 0.1978\n",
      "-0.3952\n",
      " 0.4696\n",
      " 0.0135\n",
      " 0.0860\n",
      " 0.3281\n",
      "-0.2585\n",
      " 0.0447\n",
      "-0.0338\n",
      "[torch.FloatTensor of size 20x11]\n",
      "\n",
      "\n",
      "\n",
      "Parameters of layer in the (w,b) format \n",
      "\n",
      "\n",
      "Columns 0 to 9 \n",
      "-0.3444  1.3942 -0.0215 -0.5192 -1.9942 -1.1133  0.4343 -0.2803  2.4669 -0.9925\n",
      " 1.2703 -2.4225 -0.3280  0.3313 -0.9558 -0.5990 -1.9789 -0.3090 -1.7834  0.9253\n",
      "-0.3430 -0.7367  0.8226 -0.0700 -0.1662  0.8228  0.2891  0.8371  0.3624 -0.9497\n",
      "-0.7012 -0.3133 -0.9932 -2.2598 -1.1594 -0.8560  1.9938 -0.4139 -0.6063  1.2368\n",
      "-0.4187  0.6000 -0.1015  0.2571 -0.5347  0.3605  0.7530  0.2467  0.1158 -0.1050\n",
      " 1.0820  0.2788 -0.8163  0.6685 -1.5818  1.0647  1.3787  0.1262  0.0380 -0.8065\n",
      " 0.8961  0.1002 -0.3097  0.2361 -0.1285 -0.9467  0.1360 -0.6890  1.5910 -2.1524\n",
      "-0.7983  0.3661 -1.1920 -0.0862 -0.3729  0.9411  0.4423 -0.3251 -0.0093  0.5664\n",
      " 2.1538  0.3464 -1.6982  0.9553 -1.0691  0.8652  0.1264  0.1042 -0.9291  0.5556\n",
      "-0.4160 -0.2943  0.0186 -0.5949  0.3986  0.7264 -0.3174  0.0562  0.8073  0.1197\n",
      "\n",
      "Columns 10 to 19 \n",
      "-0.1215  0.6737  1.4378 -0.7884 -0.6687 -0.9805 -0.1474  1.1964  1.2847 -0.9588\n",
      " 1.6938  2.0706 -0.5557 -0.5669 -0.5735  3.3481 -1.0153 -2.1128 -0.0957  0.8651\n",
      "-0.2194 -0.0773  0.6506  1.2366  0.9527 -0.1604 -0.0533  1.7203 -1.3089  0.4902\n",
      "-0.5049 -1.0897  0.0585  0.9783  0.6076  0.0111 -0.0733 -0.4193  0.6800 -0.1609\n",
      " 0.6208  1.4477  0.6092  0.1891 -1.4293 -1.0307 -0.0949 -0.0590 -0.6575 -1.2800\n",
      " 0.9203 -0.3898  0.2552 -0.4868 -0.6620 -0.5112  0.5398 -1.1722 -0.4038 -0.9408\n",
      " 1.3056 -0.9821 -0.0791 -1.4619 -1.9258 -0.2627  0.0497 -0.0488  0.3735  0.2529\n",
      "-0.9216 -0.4235 -0.1217 -2.3596 -1.0012 -1.5917 -1.0418 -0.4496 -1.4401  1.6886\n",
      " 3.5651 -2.4190  2.6680  1.1782 -0.6962  0.8870 -1.6835  0.5391  0.3476 -0.8976\n",
      " 0.3994  0.4721 -1.1726  1.2675 -0.1359 -1.6842  0.0079 -0.4875  1.2628 -1.7590\n",
      "\n",
      "Columns 20 to 20 \n",
      " 0.1748\n",
      " 0.0853\n",
      " 0.1351\n",
      " 0.0264\n",
      " 0.0158\n",
      "-0.0209\n",
      "-0.0605\n",
      " 0.0494\n",
      " 0.1084\n",
      " 0.0054\n",
      "[torch.FloatTensor of size 10x21]\n",
      "\n",
      "\n",
      "\n",
      "Parameters of layer in the (w,b) format \n",
      "\n",
      "\n",
      "Columns 0 to 9 \n",
      " 0.4031  1.6251 -0.5397 -0.3347  0.9002 -2.0770 -0.4717 -0.7661 -2.1192 -0.5355\n",
      "-0.1892  0.5226 -0.5739 -0.8530 -0.3100 -0.9645  0.1470  1.6639  2.2282  0.3051\n",
      "\n",
      "Columns 10 to 10 \n",
      " 0.2912\n",
      "-0.2052\n",
      "[torch.FloatTensor of size 2x11]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print ('Parameters after training')\n",
    "for p in model.parameters():\n",
    "    print ('\\n\\nParameters of layer in the (w,b) format ')\n",
    "    print torch.cat((p.w,p.b),1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error is 0.9%\n"
     ]
    }
   ],
   "source": [
    "# Testing the model\n",
    "Prediction = model.forward(test_input)[0]\n",
    "error(Prediction, test_target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot of the misclassifications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fe832a2c750>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "for idx,point in enumerate(test_input):\n",
    "    Col = 'b'* ( test_target[idx].dot(Prediction[idx]) > 0) + 'r' *( test_target[idx].dot(Prediction[idx]) <= 0)\n",
    "    plt.scatter(point[0],point[1], color = Col)\n",
    "plt.xlim(0,1)\n",
    "plt.ylim(0,1)\n",
    "plt.axis('equal')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Pytorch packages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because of wierd reasons, if you run the same model (we call it $\\texttt{pyModel}$) of the network, pytorch interprete some stuff as Variable and you can't run the our own-made model. just shutdown the notebook and start again!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "from random import randint\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from torch import Tensor\n",
    "from torch.nn import functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#seq = [Linear(2,10),'sig',Linear(10,20),'relu',Linear(20,10),'relu',Linear(10,2),'tanh']     \n",
    "pyModel = nn.Sequential(nn.Linear(2,10),nn.Sigmoid(),nn.Linear(10,20),nn.ReLU(),nn.Linear(20,10),nn.ReLU(),nn.Linear(10,2),nn.Tanh())\n",
    "\n",
    "train_input_py = Variable(train_input)\n",
    "train_target_py= Variable(train_target)\n",
    "test_input_py  = Variable(test_input)\n",
    "test_target_py = Variable(test_target)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-37-5275e2656a4f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0me\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpyModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_input_py\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_target_py\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0mpyModel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "### train the model\n",
    "epoch = 10000\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.SGD(pyModel.parameters(),lr = 0.3)\n",
    "for e in range(epoch):\n",
    "    output = pyModel(train_input_py)\n",
    "    loss = criterion(output,train_target_py)\n",
    "    pyModel.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing the model\n",
    "Prediction = pyModel(test_input_py)\n",
    "error(Prediction.data, test_target_py.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx,point in enumerate(test_input):\n",
    "    Col = 'b'* ( test_target[idx].dot(Prediction.data[idx]) > 0) + 'r' *( test_target[idx].dot(Prediction.data[idx]) <= 0)\n",
    "    plt.scatter(point[0],point[1], color = Col)\n",
    "\n",
    "plt.xlim(0,1)\n",
    "plt.ylim(0,1)\n",
    "plt.axis('equal')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final notes:\n",
    "ما الان دو تا تابع لاس داریم. یکی بر پایه ی ترشهولد و دیگری بر پایه ی ایپاک. من برای مقایسه با پای تورچ این ایپاک دار رو اضافه کردم. باید تصمیم بگیریم که کدوم رو نگه داریم. نظر من روی ایپاک داره. چون میتونیم دوباره اجراش کنیم و حتی عدد ایپاک رو هم عوض کنیم. ولی وقتی ترشهولد گذاشتیم باید اونقدری صبر کنیم که برنامه برسه به ترشهولد... وگرنه نمی ایسته برنامه..."
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
