{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import Tensor\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Auxiliary Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class layer_par:\n",
    "    \"\"\"\n",
    "    This class contains parameters of each layer. We initialize them in constructor.\n",
    "    \n",
    "    inputs:\n",
    "        dim_in      :  the input_ dimension of fully connected layer\n",
    "        dim_out     :  the output_ dimension of fully connected layer\n",
    "        \n",
    "    returns:\n",
    "        Nothing\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, dim_in, dim_out):\n",
    "        self.dim_in  = dim_in\n",
    "        self.dim_out = dim_out\n",
    "        self.b = Tensor(dim_out,1).fill_(0)         # bias of each layer\n",
    "        self.w = Tensor(dim_out,dim_in).normal_()   # w of each layer\n",
    "    # TODO: I suggest that we add activation to this as well\n",
    "    \n",
    "    \n",
    "class forward_par:    # change the name to: \"forward_par_of_layer\n",
    "    \"\"\"\n",
    "    This class keeps track of all the variables produced in forward pass of some layer. i.e, x and s.\n",
    "    \n",
    "    inputs:\n",
    "        layer       :  the layer number\n",
    "        dim_in      :  the input_ dimension of fully connected layer\n",
    "        dim_out     :  the output_ dimension of fully connected layer\n",
    "        \n",
    "    returns:\n",
    "        Nothing\n",
    "    \"\"\"\n",
    "    def __init__(self, dim_out, N):\n",
    "        self.s = Tensor(N,dim_out).fill_(0)           # s after each layer\n",
    "        self.x = Tensor(N,dim_out).fill_(0)           # x after each layer:   x = Activation (s)\n",
    "        self.N = N\n",
    "        \n",
    "class backward_par:   # or backward par\n",
    "    \"\"\"\n",
    "    This class keeps track of all the variables we need to evaluate the damn gradients for each layer...\n",
    "    \n",
    "    inputs:\n",
    "        dim_in      :  the input_ dimension of fully connected layer\n",
    "        dim_out     :  the output_ dimension of fully connected layer\n",
    "        \n",
    "    returns:\n",
    "        Nothing\n",
    "    \"\"\"\n",
    "    def __init__(self, dim_in, dim_out, N):\n",
    "        self.db = Tensor(N, dim_out,1).fill_(0)         # bias of each layer\n",
    "        self.dw = Tensor(N, dim_out,dim_in).fill_(0)    # w of each layer      # WARNING: out*in or in*out\n",
    "        self.ds = Tensor(N, dim_out).fill_(0)           # s after each layer\n",
    "        self.dx = Tensor(N, dim_out).fill_(0)\n",
    "        \n",
    "        \n",
    "class Linear:\n",
    "    \"\"\"\n",
    "    An class that contains objects which only store layar's in/out connections dimension\n",
    "    \n",
    "    input_s:\n",
    "        dim_in      :  the input_ dimension of fully connected layer\n",
    "        dim_out     :  the output_ dimension of fully connected layer\n",
    "        \n",
    "    returns:\n",
    "        Nothing \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self,dim_in, dim_out):\n",
    "        self.input_ = dim_in\n",
    "        self.output_ = dim_out          \n",
    "    # TODO: The linear is really wierd thing... all we get here is already in the upper class. we may omit this somehow\n",
    "\n",
    "        \n",
    "def Activation(code,input_):\n",
    "    \"\"\"\n",
    "    A class that specify the needed activation with respect to the following code conversion\n",
    "        0: Relu(x)\n",
    "        1: Tanh(x)\n",
    "        2: Sigmoid(x)\n",
    "    \n",
    "    This class works as functional package of pytorch\n",
    "    \n",
    "    input_s:\n",
    "        code        :  the code for each activation (0,1,2)\n",
    "        input__tensor:  the input_ tensor\n",
    "        \n",
    "    returns:\n",
    "        result      :  the output_ of requested activation function with the same shape as input_ tensor\n",
    "    \"\"\"\n",
    "    result = Tensor(input_.shape)\n",
    "    # Relu\n",
    "    if code ==0:\n",
    "        result = input_ - (input_<0).float()*input_\n",
    "    # Tanh\n",
    "    elif code ==1:\n",
    "        result = torch.tanh(input_)\n",
    "    # Sig\n",
    "    elif code ==2:\n",
    "        result = 1.0/(1 + torch.exp(-input_))\n",
    "    # linear\n",
    "    elif code ==3:\n",
    "        result = input_\n",
    "    # error\n",
    "    else: raise ValueError('Unknown Code For Activation')\n",
    "        \n",
    "    return result \n",
    "\n",
    "\n",
    "def dActivation(code,input_):\n",
    "    \"\"\"\n",
    "    A class that specify the needed derivative of activation with the same encoding convenstion\n",
    "        0: dRelu(x)\n",
    "        1: dTanh(x)\n",
    "        2: dSigmoid(x)\n",
    "    \n",
    "    This class works as functional package of pytorch\n",
    "    \n",
    "    input_s:\n",
    "        code        :  the code for each activation (0,1,2)\n",
    "        input__tensor:  the input_ tensor\n",
    "        \n",
    "    returns:\n",
    "        result      :  the output_ of requested activation function with the same shape as input_ tensor\n",
    "    \"\"\"\n",
    "    result = Tensor.new(input_)\n",
    "    # dRelu\n",
    "    if code ==0:\n",
    "        result = Tensor(input_.shape).fill_(1.0) - (input_<=0).float()*Tensor(input_.shape).fill_(1.0)\n",
    "    # dTanh\n",
    "    elif code ==1:\n",
    "        result = 1-(torch.tanh(input_))**2\n",
    "    # dSig\n",
    "    elif code ==2:\n",
    "        result = Activation(code,input_)*(1-Activation(code,input_))\n",
    "    # linear\n",
    "    elif code ==3:\n",
    "        result = Tensor(input_.shape).fill_(1.0)\n",
    "    else: raise ValueError('Unknown Code For derivative of Activation')\n",
    "    \n",
    "    return result \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss and SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(v, t):\n",
    "    l_ = torch.sum(torch.pow(v-t,2))/(len(v))\n",
    "    return l_\n",
    "\n",
    "def dloss(v,t):\n",
    "    return 2.*(v-t)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Network Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net:\n",
    "    \"\"\"\n",
    "    The network class. It has the following methods:\n",
    "        param      :  returns the parameter which is asked for. Not the data! The object... \n",
    "                        Data is accessible through object.data method)\n",
    "        make_arch  :  makes the architecture of the network by taking a sequential list of [fc1,act1,fc2,act2,...]\n",
    "    \"\"\"\n",
    "    \n",
    "    #TODO:\n",
    "    \n",
    "    \n",
    "    def __init__(self, seq,X,Y=None):\n",
    "        self.param_list    = [] # Stores parameters of each layer (W,b).                               type: layer_par\n",
    "        self.forward_list  = [] # Stores forward pass parameters of each layer (s,x).                  type: forward_par\n",
    "        self.backward_list = [] # Stores backward pass parameters of each layer (grads) (ds,dx,dW,db)  type: backward_par\n",
    "        self.grad_list     = []\n",
    "        \n",
    "        self.n_layer = 0\n",
    "        self.act_list = []      # stores the requested activation functions in codes. Elements are \"0\",\"1\" or \"2\"\n",
    "        self.make_arch(seq)     # makes the architecture based on the the list \"seq\"\n",
    "        self.N = len(X)         # nb of batch\n",
    "        \n",
    "    # a user-friendly-named method fo access w,b and s at each layer\n",
    "    def get_param_of_layer(self,layer):  \n",
    "        return self.param_list[layer]\n",
    "    \n",
    "    # a user-friendly-named method fo access grad w.r.t. w,b and s at each layer\n",
    "    def get_forward_par_of_layer(self,layer):\n",
    "        return self.forward_list[layer]\n",
    "    \n",
    "    # a user-friendly-named method fo access grad w.r.t. w,b and s at each layer\n",
    "    def get_grad_of_layer(self,layer):\n",
    "        return self.backward_list[layer]\n",
    "        \n",
    "    \n",
    "    def make_arch(self,seq):\n",
    "        \n",
    "        seq_len = len(seq)                  # number of layer *2 (because of the activations...)\n",
    "        self.n_layer = seq_len/2            # I just want to have it :)\n",
    "        \n",
    "        for layer in range (0,seq_len,2):\n",
    "            \n",
    "            # seq[layer] is an instance of object \"Linear\". Here we get the in/out dim of the layer\n",
    "            dim_in, dim_out = seq[layer].input_ , seq[layer].output_ \n",
    "            \n",
    "            # initialize the weights of layer\n",
    "            self.param_list.append (layer_par(dim_in, dim_out) ) \n",
    "            \n",
    "            # activation recognition : encode activations in \"act_list\"\n",
    "            if seq[layer+1]=='relu':\n",
    "                self.act_list.append(0)\n",
    "            elif seq[layer+1]=='tanh':\n",
    "                self.act_list.append(1)\n",
    "            elif seq[layer+1]=='sig':\n",
    "                self.act_list.append(2)\n",
    "            elif seq[layer+1]=='lin':\n",
    "                self.act_list.append(3)\n",
    "            else: raise ValueError('Unknown Activation')\n",
    "                \n",
    "    \n",
    "    \n",
    "    def forward(self,X): \n",
    "        x=X\n",
    "        for layer, prm in enumerate(self.param_list):       # prm = param_list[layer]  \n",
    "            s = (x.mm(prm.w.t()) + prm.b.t())           # written consistant for batch :  s = (Wx+b).t()  (N,d_out) \n",
    "            x = Activation(self.act_list[layer], s)     # size = (N,d_out)       \n",
    "        return x,s\n",
    "    \n",
    "    \n",
    "    def backward (self,X,Y):\n",
    "        self.backward_list = range(self.n_layer+1)     # this list will be filled with backward_par objects\n",
    "        \n",
    "        # add X0 to the forward list\n",
    "        self.forward_list.append( forward_par(self.param_list[0].dim_in, self.N) )    # Note that dim = dim_in for inputs\n",
    "        self.forward_list[0].x =X    # x0\n",
    "        #self.forward_list[0].s =X    # s0 is set to be x0\n",
    "        \n",
    "        \n",
    "        \n",
    "        # this computes forward\n",
    "        x=X\n",
    "        for layer, prm in enumerate(self.param_list):       # parameter = param[layer]  \n",
    "            self.forward_list.append( forward_par(prm.dim_out, self.N) )\n",
    "\n",
    "            s = (x.mm(prm.w.t()) + prm.b.t())           # written consistant for batch :  s = (Wx+b).t()\n",
    "            self.forward_list[layer+1].s = s             \n",
    "            x = Activation(self.act_list[layer], s)    \n",
    "            self.forward_list[layer+1].x = x\n",
    "            \n",
    "            \n",
    "            \n",
    "        # this computes backward\n",
    "        for layer in range (self.n_layer,0,-1):\n",
    "            #print ('layer is {}'.format(layer))\n",
    "            # this guy makes a backward_par (dim_in,dim_out, N) at component \"layer\" of backward_list\n",
    "            self.backward_list[layer] = backward_par( self.param_list[layer-1].dim_in , self.param_list[layer-1].dim_out ,self.N)\n",
    "            \n",
    "            #dl/dx \n",
    "            # WARNING\n",
    "            if layer == self.n_layer:\n",
    "                self.backward_list[layer].dx = dloss(x,Y) \n",
    "            else:\n",
    "                #self.backward_list[layer].dx = self.param_list[layer].w.t().mm(self.backward_list[layer+1].ds)\n",
    "                self.backward_list[layer].dx = self.backward_list[layer+1].ds.mm(self.param_list[layer].w)\n",
    "                \n",
    "                \n",
    "            #dl/ds\n",
    "            #print (self.backward_list[layer].dx.shape)\n",
    "            #print (dActivation(self.act_list[layer-1], self.forward_list[layer].s ).shape)\n",
    "            self.backward_list[layer].ds = self.backward_list[layer].dx * dActivation(self.act_list[layer-1], self.forward_list[layer].s ) \n",
    "            \n",
    "            #dl/dw\n",
    "            #print( self.forward_list[layer].x.shape)\n",
    "            # x:  X of layer (l-1) -> x.size = N*d_in\n",
    "            # ds: dL/ds of layer (l) -> ds.size = N*d_out\n",
    "            \n",
    "            arash=self.backward_list[layer].ds.unsqueeze(1)\n",
    "            ehsan=self.forward_list[layer-1].x.unsqueeze(2)\n",
    "            \"\"\"\n",
    "            print(arash.shape)\n",
    "            print(ehsan.shape)\n",
    "            \"\"\"\n",
    "            #print (self.backward_list[layer].ds.shape)\n",
    "            self.backward_list[layer].dw = arash*ehsan\n",
    "            #print ('said obi-wan kenobi')\n",
    "            \n",
    "            #dl/db\n",
    "            self.backward_list[layer].db = self.backward_list[layer].ds\n",
    "            \n",
    "            #print(self.backward_list[layer].db)\n",
    "            \"\"\"\n",
    "            print (self.backward_list[layer].ds.shape)\n",
    "            print (self.backward_list[layer].dw.shape)\n",
    "            print (self.backward_list[layer].db.shape)\n",
    "            print (self.backward_list[layer].dx.shape)\n",
    "            print ('hello there')\n",
    "\"\"\"\n",
    "        # summing all batch grads\n",
    "        for layer in range (1,self.n_layer+1):\n",
    "            #print (layer,self.n_layer)\n",
    "            \n",
    "            #print (self.backward_list[layer])\n",
    "            #print (self.backward_list[layer].dw)\n",
    "            self.grad_list.append(layer_par(self.param_list[layer-1].dim_in,self.param_list[layer-1].dim_out) )\n",
    "            \n",
    "            self.grad_list[layer-1].b = self.backward_list[layer].db.sum(0)/self.N   # watchout! these are grad!\n",
    "            self.grad_list[layer-1].w = self.backward_list[layer].dw.sum(0)/self.N   # watchout! these are grad!\n",
    "            \n",
    "        return self.grad_list\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Draft, tests, and other stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = Tensor(7).normal_()\n",
    "X = torch.cat((x.view(1,-1),x.view(1,-1)*3,x.view(1,-1)*x.view(1,-1)),0)\n",
    "Y= Tensor(3,10).normal_();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq = [Linear(7,2),'lin',Linear(2,5),'relu',Linear(5,10),'lin']    \n",
    "model = Net(seq,X,Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "win = model.backward(X,Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "\n",
       "Columns 0 to 7 \n",
       " -0.2330  -1.6584   0.4471  -0.2246  -0.2770   0.7003  -0.7584  -0.0651\n",
       "  0.0000   0.0000   0.0000   0.0000   0.0000   0.0000   0.0000   0.0000\n",
       " -0.1047  -0.7456   0.2010  -0.1010  -0.1245   0.3149  -0.3410  -0.0293\n",
       " -3.5068 -47.0963  27.6213 -16.2701  10.3904   1.4681  -4.2070 -17.2292\n",
       " -1.2886 -18.6242  11.3941  -6.7463   4.6853  -0.0011  -1.1164  -7.2989\n",
       "\n",
       "Columns 8 to 9 \n",
       " -0.0849   0.4276\n",
       "  0.0000   0.0000\n",
       " -0.0382   0.1922\n",
       " 13.3894 -26.2503\n",
       "  5.7937 -11.5929\n",
       "[torch.FloatTensor of size 5x10]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "win[2].w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(\n",
       " \n",
       " Columns 0 to 7 \n",
       "   2.2719  -0.2327  -0.8425  -3.0341  -9.4195  -4.0008  -5.4038   4.0141\n",
       "   6.8156  -0.6981  -2.5276  -9.1022 -28.2586 -12.0025 -16.2113  12.0423\n",
       "  -6.5716   7.6118   7.8077  -5.4867 -14.6678  -7.0281   6.7408   7.9006\n",
       " \n",
       " Columns 8 to 9 \n",
       "   3.3794  -1.8302\n",
       "  10.1382  -5.4907\n",
       "  -2.4201   0.6710\n",
       " [torch.FloatTensor of size 3x10], \n",
       " \n",
       " Columns 0 to 7 \n",
       "   2.2719  -0.2327  -0.8425  -3.0341  -9.4195  -4.0008  -5.4038   4.0141\n",
       "   6.8156  -0.6981  -2.5276  -9.1022 -28.2586 -12.0025 -16.2113  12.0423\n",
       "  -6.5716   7.6118   7.8077  -5.4867 -14.6678  -7.0281   6.7408   7.9006\n",
       " \n",
       " Columns 8 to 9 \n",
       "   3.3794  -1.8302\n",
       "  10.1382  -5.4907\n",
       "  -2.4201   0.6710\n",
       " [torch.FloatTensor of size 3x10])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.forward(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "b= Tensor(2,4).normal_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'a' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-8201c27d19b7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0ma\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'a' is not defined"
     ]
    }
   ],
   "source": [
    "a*b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3, 4]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "range(1,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "a= Tensor (3,1,5).normal_()\n",
    "b= Tensor (3,2,1).normal_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "(0 ,.,.) = \n",
       "  2.6336  0.5373  0.3234 -0.8952  2.7713\n",
       "  0.0638  0.0130  0.0078 -0.0217  0.0671\n",
       "\n",
       "(1 ,.,.) = \n",
       "  0.0563 -0.5344 -0.3692  0.5568  0.3351\n",
       " -0.1579  1.4984  1.0352 -1.5614 -0.9397\n",
       "\n",
       "(2 ,.,.) = \n",
       " -1.9301 -0.2083  1.3914  2.1775 -1.8134\n",
       "  1.9941  0.2152 -1.4375 -2.2496  1.8735\n",
       "[torch.FloatTensor of size 3x2x5]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a*b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "(0 ,.,.) = \n",
       " -2.0070 -0.4095 -0.2464  0.6822 -2.1119\n",
       "\n",
       "(1 ,.,.) = \n",
       "  0.1641 -1.5579 -1.0764  1.6235  0.9771\n",
       "\n",
       "(2 ,.,.) = \n",
       " -1.1079 -0.1195  0.7986  1.2499 -1.0409\n",
       "[torch.FloatTensor of size 3x1x5]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "(0 ,.,.) = \n",
       " -1.3122\n",
       " -0.0318\n",
       "\n",
       "(1 ,.,.) = \n",
       "  0.3430\n",
       " -0.9618\n",
       "\n",
       "(2 ,.,.) = \n",
       "  1.7422\n",
       " -1.7999\n",
       "[torch.FloatTensor of size 3x2x1]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
