{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import Tensor\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Auxiliary Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class layer_par:\n",
    "    \"\"\"\n",
    "    This class contains parameters of each layer. We initialize them in constructor.\n",
    "    \n",
    "    inputs:\n",
    "        dim_in      :  the input_ dimension of fully connected layer\n",
    "        dim_out     :  the output_ dimension of fully connected layer\n",
    "        \n",
    "    returns:\n",
    "        Nothing\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, dim_in, dim_out):\n",
    "        self.b = Tensor(dim_out,1).fill_(0)         # bias of each layer\n",
    "        self.w = Tensor(dim_out,dim_in).normal_()   # w of each layer\n",
    "        self.s = Tensor(dim_out).fill_(0)           # s after each layer\n",
    "        self.x = Tensor(dim_out).fill_(0)           # x after each layer:   x = Activation (s)\n",
    "    # TODO: I suggest that we add activation to this as well\n",
    "    \n",
    "class grad_wrt:\n",
    "    \"\"\"\n",
    "    This class keeps track of all the variables we need to evaluate the damn gradients for each layer...\n",
    "    \n",
    "    inputs:\n",
    "        dim_in      :  the input_ dimension of fully connected layer\n",
    "        dim_out     :  the output_ dimension of fully connected layer\n",
    "        \n",
    "    returns:\n",
    "        Nothing\n",
    "    \"\"\"\n",
    "    def __init__(self, dim_in, dim_out):\n",
    "        self.db = Tensor(dim_out,1).fill_(0)         # bias of each layer\n",
    "        self.dw = Tensor(dim_out,dim_in).fill_(0)   # w of each layer\n",
    "        self.ds = Tensor(dim_out).fill_(0)           # s after each layer\n",
    "        self.dx = Tensor(dim_out).fill_(0)\n",
    "    \n",
    "    def call_grad(self):\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "class Linear:\n",
    "    \"\"\"\n",
    "    An class that contains objects which only store layar's in/out connections dimension\n",
    "    \n",
    "    input_s:\n",
    "        dim_in      :  the input_ dimension of fully connected layer\n",
    "        dim_out     :  the output_ dimension of fully connected layer\n",
    "        \n",
    "    returns:\n",
    "        Nothing \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self,dim_in, dim_out):\n",
    "        self.input_ = dim_in\n",
    "        self.output_ = dim_out          \n",
    "    # TODO: The linear is really wierd thing... all we get here is already in the upper class. we may omit this somehow\n",
    "\n",
    "        \n",
    "def Activation(code,input_):\n",
    "    \"\"\"\n",
    "    A class that specify the needed activation with respect to the following code conversion\n",
    "        0: Relu(x)\n",
    "        1: Tanh(x)\n",
    "        2: Sigmoid(x)\n",
    "    \n",
    "    This class works as functional package of pytorch\n",
    "    \n",
    "    input_s:\n",
    "        code        :  the code for each activation (0,1,2)\n",
    "        input__tensor:  the input_ tensor\n",
    "        \n",
    "    returns:\n",
    "        result      :  the output_ of requested activation function with the same shape as input_ tensor\n",
    "    \"\"\"\n",
    "    result = Tensor(input_.shape)\n",
    "    # Relu\n",
    "    if code ==0:\n",
    "        result = input_ - (input_<0).float()*input_\n",
    "    # Tanh\n",
    "    elif code ==1:\n",
    "        result = torch.tanh(input_)\n",
    "    # Sig\n",
    "    elif code ==2:\n",
    "        result = 1.0/(1 + torch.exp(-input_))\n",
    "    # error\n",
    "    else: raise ValueError('Unknown Code For Activation')\n",
    "        \n",
    "    return result \n",
    "\n",
    "\n",
    "def dActivation(code,input_):\n",
    "    \"\"\"\n",
    "    A class that specify the needed derivative of activation with the same encoding convenstion\n",
    "        0: dRelu(x)\n",
    "        1: dTanh(x)\n",
    "        2: dSigmoid(x)\n",
    "    \n",
    "    This class works as functional package of pytorch\n",
    "    \n",
    "    input_s:\n",
    "        code        :  the code for each activation (0,1,2)\n",
    "        input__tensor:  the input_ tensor\n",
    "        \n",
    "    returns:\n",
    "        result      :  the output_ of requested activation function with the same shape as input_ tensor\n",
    "    \"\"\"\n",
    "    result = Tensor.new(input_)\n",
    "    # dRelu\n",
    "    if code ==0:\n",
    "        result = Tensor(input_.shape).fill_(1.0) - (input_<0).float()*Tensor(input_.shape).fill_(1.0)\n",
    "    # dTanh\n",
    "    elif code ==1:\n",
    "        result = 1-(torch.tanh(input_))**2\n",
    "    # dSig\n",
    "    elif code ==2:\n",
    "        result = Activation(code,input_)*(1-Activation(code,input_))\n",
    "    # error\n",
    "    else: raise ValueError('Unknown Code For derivative of Activation')\n",
    "    \n",
    "    return result "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Network Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net:\n",
    "    \"\"\"\n",
    "    The network class. It has the following methods:\n",
    "        param      :  returns the parameter which is asked for. Not the data! The object... \n",
    "                        Data is accessible through object.data method)\n",
    "        make_arch  :  makes the architecture of the network by taking a sequential list of [fc1,act1,fc2,act2,...]\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, seq=[]):\n",
    "        self.param_list = []    # Stores the parameters of each layer.Elements are \"layer_par\" objects\n",
    "        self.act_list = []      # stores the requested activation functions in codes. Elements are \"0\",\"1\" or \"2\"\n",
    "        self.grad_list = []     # stores grad wrt parameters at each layer. Elemenst are matrices of size (d_in,d_out +1)\n",
    "        self.make_arch(seq)     # makes the architecture based on the the list \"seq\"\n",
    "        \n",
    "    \n",
    "    # a user-friendly-named method fo access w,b and s at each layer\n",
    "    def get_param_of_layer(self,layer):      \n",
    "        return self.param_list[layer]\n",
    "    \n",
    "    # a user-friendly-named method fo access grad w.r.t. w,b and s at each layer\n",
    "    def get_grad_of_layer(self,layer):\n",
    "        pass\n",
    "    \n",
    "    # a user-friendly-named method fo access w,b and s at each layer\n",
    "    def make_arch(self,seq):\n",
    "        seq_len = len(seq)                  # number of layer *2 (because of the activations...)\n",
    "        \n",
    "        for layer in range (0,seq_len,2):\n",
    "            \n",
    "            # seq[layer] is an instance of object \"Linear\". Here we get the in/out dim of the layer\n",
    "            dim_in, dim_out = seq[layer].input_ , seq[layer].output_ \n",
    "            \n",
    "            # initialize the weights of layer\n",
    "            self.param_list.append ( layer_par(dim_in, dim_out) ) \n",
    "            \n",
    "            # activation recognition : encode activations in \"act_list\"\n",
    "            if seq[layer+1]=='relu':\n",
    "                self.act_list.append(0)\n",
    "            elif seq[layer+1]=='tanh':\n",
    "                self.act_list.append(1)\n",
    "            elif seq[layer+1]=='sig':\n",
    "                self.act_list.append(2)\n",
    "            else: raise ValueError('Unknown Activation')\n",
    "                \n",
    "            \n",
    "    def forward(self,x): \n",
    "        for layer, prm in enumerate(self.param_list):       # parameter = param[layer]  \n",
    "            prm.s = (x.mm(prm.w.t()) + prm.b.t())                  # written consistant for batch\n",
    "            x = Activation(self.act_list[layer],prm.s)\n",
    "        return x\n",
    "        \n",
    "    def backward ():\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss and SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(v, t):\n",
    "    l_ = torch.sum(torch.pow(v-t,2))\n",
    "    return l_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Draft, tests, and other stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq = [Linear(7,2),'tanh',Linear(2,3),'relu']    \n",
    "model = Net(seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "-0.4180  0.7239  3.1533  0.5672 -2.0508 -0.0750  1.0284\n",
       "-1.2541  2.1717  9.4600  1.7017 -6.1525 -0.2251  3.0853\n",
       " 0.1748  0.5240  9.9435  0.3218  4.2059  0.0056  1.0577\n",
       "[torch.FloatTensor of size 3x7]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = Tensor(7).normal_()\n",
    "X = torch.cat((x.view(1,-1),x.view(1,-1)*3,x.view(1,-1)*x.view(1,-1)),0)\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       " 0.3425  0.0122  0.0000\n",
       " 0.4596  0.0000  0.0000\n",
       " 0.0000  0.6200  0.0000\n",
       "[torch.FloatTensor of size 3x3]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.forward(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "w1= model.get_param_of_layer(0).w\n",
    "b1= model.get_param_of_layer(0).b\n",
    "s1= model.get_param_of_layer(0).s\n",
    "w2= model.get_param_of_layer(1).w\n",
    "b2= model.get_param_of_layer(1).b\n",
    "s2= model.get_param_of_layer(1).s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "S1 = X.mm(w1.t())+b1.t()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "S2 = Activation(1,S1).mm(w2.t())+b2.t()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       " 0.3425  0.0122 -0.6994\n",
       " 0.4596 -0.0652 -0.7664\n",
       "-0.1775  0.6200 -0.9608\n",
       "[torch.FloatTensor of size 3x3]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "S2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "-0.4247 -1.3250 -0.0681 -0.5092 -0.5179  0.0591  0.5718\n",
       "-0.3701 -1.6274 -0.7035  0.3733 -0.4113 -0.5786  0.4245\n",
       " 0.0884 -0.0916  2.0080  0.2671 -2.0022  0.3206  1.6533\n",
       "[torch.FloatTensor of size 3x7]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y=Tensor(X.shape).normal_()\n",
    "Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "281.9885527333354"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss(X,Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "-0.4180  0.7239  3.1533  0.5672 -2.0508 -0.0750  1.0284\n",
       "-1.2541  2.1717  9.4600  2.0000 -6.1525 -0.2251  3.0853\n",
       " 0.1748  0.5240  9.9435  0.3218  4.2059  0.0056  1.0577\n",
       "[torch.FloatTensor of size 3x7]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "Z = Tensor(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "Z[1,3]=2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "-0.4180  0.7239  3.1533  0.5672 -2.0508 -0.0750  1.0284\n",
       "-1.2541  2.1717  9.4600  2.0000 -6.1525 -0.2251  3.0853\n",
       " 0.1748  0.5240  9.9435  0.3218  4.2059  0.0056  1.0577\n",
       "[torch.FloatTensor of size 3x7]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "Z= Tensor(3,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "q= Tensor(4,7).normal_()\n",
    "z= Tensor(q.shape)\n",
    "Z= Tensor(q.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "    0     1     1     1     0     0     1\n",
       "    0     1     1     1     0     0     1\n",
       "    1     1     1     1     1     1     1\n",
       "[torch.FloatTensor of size 3x7]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dActivation(0,X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       " 0.8438  0.6164  0.0073  0.7365  0.0640  0.9944  0.4020\n",
       " 0.2785  0.0506  0.0000  0.0707  0.0000  0.9510  0.0083\n",
       " 0.9701  0.7688  0.0000  0.9032  0.0009  1.0000  0.3841\n",
       "[torch.FloatTensor of size 3x7]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dActivation(1,X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       " 0.2394  0.2199  0.0393  0.2309  0.1010  0.2496  0.1940\n",
       " 0.1727  0.0919  0.0001  0.1050  0.0021  0.2469  0.0418\n",
       " 0.2481  0.2336  0.0000  0.2436  0.0145  0.2500  0.1913\n",
       "[torch.FloatTensor of size 3x7]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dActivation(2,X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "-0.4180  0.7239  3.1533  0.5672 -2.0508 -0.0750  1.0284\n",
       "-1.2541  2.1717  9.4600  2.0000 -6.1525 -0.2251  3.0853\n",
       " 0.1748  0.5240  9.9435  0.3218  4.2059  0.0056  1.0577\n",
       "[torch.FloatTensor of size 3x7]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
