{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import Tensor\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Suggestion on Naming\n",
    "\n",
    "    layer_par    :  Layer    (it's a class)\n",
    "    forward_par  :  Forward  (class)\n",
    "    backward_par :  Backward (class)\n",
    "    \n",
    "    \n",
    "    make_arch    :  makeArhc (it's a method in Net class)\n",
    "    forward_list :  fw_list  (attribute list)\n",
    "    backward_list:  bw_list  (attribute list)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Auxiliary Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear:\n",
    "    \"\"\"\n",
    "    An class that contains objects which only store layar's in/out connections dimension\n",
    "    \n",
    "    input_s:\n",
    "        dim_in      :  the input_ dimension of fully connected layer\n",
    "        dim_out     :  the output_ dimension of fully connected layer\n",
    "        \n",
    "    returns:\n",
    "        Nothing \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self,dim_in, dim_out):\n",
    "        self.input_ = dim_in\n",
    "        self.output_ = dim_out          \n",
    "    # TODO: The linear is really wierd thing... all we get here is already in the upper class. we may omit this somehow\n",
    "\n",
    "#---------------------------------------------------------------------------------------------\n",
    "        \n",
    "def Activation(code,input_):\n",
    "    \"\"\"\n",
    "    A class that specify the needed activation with respect to the following code conversion\n",
    "        0: Relu(x)\n",
    "        1: Tanh(x)\n",
    "        2: Sigmoid(x)\n",
    "    \n",
    "    This class works as functional package of pytorch\n",
    "    \n",
    "    input_s:\n",
    "        code        :  the code for each activation (0,1,2)\n",
    "        input__tensor:  the input_ tensor\n",
    "        \n",
    "    returns:\n",
    "        result      :  the output_ of requested activation function with the same shape as input_ tensor\n",
    "    \"\"\"\n",
    "    result = Tensor(input_.shape)\n",
    "    # Relu\n",
    "    if code ==0:\n",
    "        result = input_ - (input_<0).float()*input_\n",
    "    # Tanh\n",
    "    elif code ==1:\n",
    "        result = torch.tanh(input_)\n",
    "    # Sig\n",
    "    elif code ==2:\n",
    "        result = 1.0/(1 + torch.exp(-input_))\n",
    "    # linear\n",
    "    elif code ==3:\n",
    "        result = input_\n",
    "    # error\n",
    "    else: raise ValueError('Unknown Code For Activation')\n",
    "        \n",
    "    return result \n",
    "\n",
    "#---------------------------------------------------------------------------------------------\n",
    "\n",
    "def dActivation(code,input_):\n",
    "    \"\"\"\n",
    "    A class that specify the needed derivative of activation with the same encoding convenstion\n",
    "        0: dRelu(x)\n",
    "        1: dTanh(x)\n",
    "        2: dSigmoid(x)\n",
    "    \n",
    "    This class works as functional package of pytorch\n",
    "    \n",
    "    input_s:\n",
    "        code        :  the code for each activation (0,1,2)\n",
    "        input__tensor:  the input_ tensor\n",
    "        \n",
    "    returns:\n",
    "        result      :  the output_ of requested activation function with the same shape as input_ tensor\n",
    "    \"\"\"\n",
    "    result = Tensor.new(input_)\n",
    "    # dRelu\n",
    "    if code ==0:\n",
    "        result = Tensor(input_.shape).fill_(1.0) - (input_<=0).float()*Tensor(input_.shape).fill_(1.0)\n",
    "    # dTanh\n",
    "    elif code ==1:\n",
    "        result = 1-(torch.tanh(input_))**2\n",
    "    # dSig\n",
    "    elif code ==2:\n",
    "        result = Activation(code,input_)*(1-Activation(code,input_))\n",
    "    # linear\n",
    "    elif code ==3:\n",
    "        result = Tensor(input_.shape).fill_(1.0)\n",
    "    else: raise ValueError('Unknown Code For derivative of Activation')\n",
    "    \n",
    "    return result \n",
    "\n",
    "#---------------------------------------------------------------------------------------------\n",
    "\n",
    "class layer_par:   # Layer\n",
    "    \"\"\"\n",
    "    This class contains parameters of each layer. We initialize them in constructor.\n",
    "    \n",
    "    inputs:\n",
    "        dim_in      :  the input_ dimension of fully connected layer\n",
    "        dim_out     :  the output_ dimension of fully connected layer\n",
    "        \n",
    "    returns:\n",
    "        Nothing\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, dim_in, dim_out):\n",
    "        self.dim_in  = dim_in\n",
    "        self.dim_out = dim_out\n",
    "        self.b = Tensor(dim_out,1).fill_(0)         # bias of each layer\n",
    "        self.w = Tensor(dim_out,dim_in).normal_()   # w of each layer\n",
    "    # TODO: I suggest that we add activation to this as well\n",
    "\n",
    "        \n",
    "#---------------------------------------------------------------------------------------------\n",
    "    \n",
    "class forward_par:    # change the name to: \"ForwardAns\n",
    "    \"\"\"\n",
    "    This class keeps track of all the variables produced in forward pass of some layer. i.e, x and s.\n",
    "    \n",
    "    inputs:\n",
    "        N           :  number of data\n",
    "        dim_in      :  the input_ dimension of fully connected layer\n",
    "        dim_out     :  the output_ dimension of fully connected layer\n",
    "        \n",
    "    returns:\n",
    "        Nothing\n",
    "    \"\"\"\n",
    "    def __init__(self, dim_out, N):\n",
    "        self.s = Tensor(N,dim_out).fill_(0)           # s after each layer\n",
    "        self.x = Tensor(N,dim_out).fill_(0)           # x after each layer:   x = Activation (s)\n",
    "        #self.N = N\n",
    "        \n",
    "#---------------------------------------------------------------------------------------------\n",
    "\n",
    "class backward_par:   # or backward par     #BackwardAns\n",
    "    \"\"\"\n",
    "    This class keeps track of all the variables we need to evaluate the damn gradients for each layer...\n",
    "    \n",
    "    inputs:\n",
    "        N           :  number of data\n",
    "        dim_in      :  the input_ dimension of fully connected layer\n",
    "        dim_out     :  the output_ dimension of fully connected layer\n",
    "\n",
    "    \"\"\"\n",
    "    #TODO: check the size of db\n",
    "    def __init__(self, dim_in, dim_out, N):\n",
    "        self.db = Tensor(N, dim_out,1).fill_(0)         # dL/db\n",
    "        self.dw = Tensor(N, dim_out,dim_in).fill_(0)    # dL/dw      # WARNING: out*in or in*out\n",
    "        self.ds = Tensor(N, dim_out).fill_(0)           # dL/ds\n",
    "        self.dx = Tensor(N, dim_out).fill_(0)           # dL/dx\n",
    "        \n",
    "\n",
    "#---------------------------------------------------------------------------------------------\n",
    "\n",
    "class Param:\n",
    "    \"\"\"\n",
    "    This class contains parameters of each layer. We initialize them in constructor.\n",
    "    \n",
    "    inputs:\n",
    "        dim_in      :  the input_ dimension of fully connected layer\n",
    "        dim_out     :  the output_ dimension of fully connected layer\n",
    "        b           :  bias vector\n",
    "        w           :  weight matrix\n",
    "        db          :  grad wrt bias\n",
    "        dw          :  grad wrt weight matrix\n",
    "    modules:\n",
    "        data        :  returns the parameters as one tensor \n",
    "        grad        :  returns the grad of Loss wrt to parameter as one tensor\n",
    "    \"\"\"\n",
    "    def __init__(self, dim_in, dim_out):\n",
    "        self.dim_in  = dim_in\n",
    "        self.dim_out = dim_out\n",
    "        self.b = Tensor(dim_out,1).fill_(0)          # bias of each layer\n",
    "        self.w = Tensor(dim_out,dim_in).normal_()    # w of each layer\n",
    "        self.db = Tensor(dim_out,1).fill_(0)         # dL/dbias of each layer\n",
    "        self.dw = Tensor(dim_out,dim_in).normal_()   # dL/dw of each layer\n",
    "    \n",
    "    \n",
    "    def data(self):\n",
    "        return (torch.cat((self.w, self.b),1))\n",
    "    \n",
    "    def grad(self):\n",
    "        return (torch.cat((self.dw.t(), self.db),1))\n",
    "    \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss: MSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(v, t):\n",
    "    l_ = torch.sum(torch.pow(v-t,2))/(len(v))\n",
    "    return l_\n",
    "\n",
    "def dloss(v,t):\n",
    "    return 2.*(v-t)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Network Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net:\n",
    "    \"\"\"\n",
    "    The network class. It has the following methods:\n",
    "        param      :  returns the parameter which is asked for. Not the data! The object... \n",
    "                        Data is accessible through object.data method)\n",
    "        make_arch  :  makes the architecture of the network by taking a sequential list of [fc1,act1,fc2,act2,...]\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, seq,X,Y=None):\n",
    "        \n",
    "        self.N = len(X)                                     # nb of batch\n",
    "        self.n_layer =len(seq)//2                           # number of layer  ((*/2) is because of the activations...)\n",
    "        \n",
    "        self.param_list    = list(range(self.n_layer))      # Stores parameters of each layer (W,b).                               type: layer_par\n",
    "        self.forward_list  = list(range(self.n_layer+1))    # Stores forward pass parameters of each layer (s,x).\n",
    "        self.backward_list = list(range(self.n_layer+1))    # Stores backward pass parameters of each layer (grads:dL/ds, dL/dx, dL/dW, dL/db)\n",
    "        #self.grad_list     = list(range(self.n_layer))      # Stores dL/dW and dL/db for all batch data at each layer\n",
    "        \n",
    "        self.act_list = list(range(self.n_layer))           # stores the requested activation functions in codes. Elements are \"0\",\"1\" or \"2\"\n",
    "        self.make_arch(seq)                                 # makes the architecture based on the the list \"seq\"\n",
    "        \n",
    "        \n",
    "        # the lists below keep the following objects: forward_par, backward_par for each single layer.\n",
    "        # Hence these list are as long as the nuber of layers +1 (for input).\n",
    "        \n",
    "        # the lists below, are as long as number of layers. They store the following objects:layer_par, type of \n",
    "        # activation requested by user.  \n",
    "        \n",
    "        # this method, builds the network and it is called automatically by constructing Net model.\n",
    "     \n",
    "  \n",
    "    \n",
    "    def make_arch(self,seq):    # makeArch\n",
    "        \"\"\"\n",
    "        This function fills param_list and act_list and also, evaluates number of layers.\n",
    "        \n",
    "        input:\n",
    "            seq : a list that contains both activation and layers in a sequential manner\n",
    "                  Example: [Linear(3,5), 'relu', Linear(5,64), 'Sig', Linear(64,1), 'Tanh']\n",
    "        \"\"\"\n",
    "        \n",
    "        seq_len = len(seq)                  # number of layer *2 (because of the activations...)\n",
    "        for layer in list(range(0,seq_len,2)):\n",
    "             \n",
    "            # seq[layer] is an instance of object \"Linear\". Here we get the in/out dim of the layer\n",
    "            dim_in, dim_out = seq[layer].input_ , seq[layer].output_ \n",
    "            \n",
    "            # making a new laye_par instance and adding it to the param_list\n",
    "            self.param_list[int(layer/2)]= Param(dim_in, dim_out) \n",
    "            \n",
    "            # activation recognition : encode activations in \"act_list\"\n",
    "            if seq[layer+1]=='relu':\n",
    "                self.act_list.append(0)\n",
    "            elif seq[layer+1]=='tanh':\n",
    "                self.act_list.append(1)\n",
    "            elif seq[layer+1]=='sig':\n",
    "                self.act_list.append(2)\n",
    "            elif seq[layer+1]=='lin':\n",
    "                self.act_list.append(3)\n",
    "            else: raise ValueError('Unknown Activation')\n",
    "                    \n",
    "    \n",
    "    def forward(self,X): \n",
    "        \"\"\"\n",
    "        This method evaluates the forward pass and returns the values. This function is written such that it take a\n",
    "        batch input and returns the forward pass of the batch.\n",
    "        \n",
    "        input: \n",
    "            X    :   a tensor of size(B, d_in) \n",
    "            \n",
    "        returns:\n",
    "            s    :   a tensor of size(B, d_out) \n",
    "            x    :   a tensor of size(B, d_out) \n",
    "        \"\"\"\n",
    "        \n",
    "        x=X\n",
    "        for layer, prm in enumerate(self.param_list):   # layer = [0,1,2,...,nb_layer -1]  ;  prm = param_list[layer]\n",
    "            s = (x.mm(prm.w.t()) + prm.b.t())           # written consistant for batch :  s = (Wx+b).t()  (N,d_out) \n",
    "            x = Activation(self.act_list[layer], s)     # size = (N,d_out)       \n",
    "        return x,s\n",
    "    \n",
    "        \"\"\"\n",
    "        Hint:\n",
    "            prm.w.shape = (d_out, d_in)\n",
    "            prm.b.shpae = (d_out, 1)\n",
    "            X.shape     = (N,d_in)\n",
    "            x.shape     = (N,d_out)\n",
    "            s.shape     = (N.d_out)\n",
    "        \"\"\"\n",
    "    \n",
    "    \n",
    "    \n",
    "    def backward (self,X,Y):\n",
    "        \"\"\"\n",
    "        This method fill both forward_list and backward_list by constructing instances of forward_par and backward_par.\n",
    "        The object forward_par, contains (s,x), and the object backward_par contains the gradients with respect to para-\n",
    "        meteres of each layer.\n",
    "        \n",
    "        These object contain the gradients or parameters of to all batch data. The sum of all parameters determines the\n",
    "        total gradient of the batch. This summation is stored in the grad_list.\n",
    "        \n",
    "        input:\n",
    "            X        :   training set of size (B,d)\n",
    "            Y        :   target set of size (B,d)\n",
    "            \n",
    "        returns:\n",
    "            grad_list:   a list of objects wich contain total gradient of the loss function wrt W and b for batch data\n",
    "        \"\"\"\n",
    "        \n",
    "        # add X0 to the forward list\n",
    "        self.forward_list[0]= forward_par(self.param_list[0].dim_in, self.N)    # Note that dim = dim_in for inputs\n",
    "        self.forward_list[0].x =X    # x0\n",
    "        self.forward_list[0].s =X    # s0 is set to be x0\n",
    "        \n",
    "        \n",
    "        # this computes forward pass and saves s and x\n",
    "        x=X\n",
    "        for layer, prm in enumerate(self.param_list): # layer = [0,1,2,...,nb_layer -1]  ; prm = param_list[layer]  \n",
    "            self.forward_list[layer+1]= forward_par(prm.dim_out, self.N)  # constructing forward parameters\n",
    "\n",
    "            s = (x.mm(prm.w.t()) + prm.b.t())         # written consistant for batch: s.shape = (N,d_out)\n",
    "            self.forward_list[layer+1].s = s             \n",
    "            x = Activation(self.act_list[layer], s)    \n",
    "            self.forward_list[layer+1].x = x\n",
    "         \n",
    "        \n",
    "        # this computes backward\n",
    "        for layer in list(range (self.n_layer,0,-1)): # layer=[nb_layer, nb_layer-1 , ..., 1]\n",
    "            \n",
    "            # this guy makes a backward_par (dim_in,dim_out, N) at component \"layer\" of backward_list\n",
    "            self.backward_list[layer] = backward_par( self.param_list[layer-1].dim_in , self.param_list[layer-1].dim_out ,self.N)\n",
    "            \"\"\"\n",
    "            hint: each object in the list above has the following attributes:\n",
    "                db = Tensor(N, dim_out,1)         # dL/db\n",
    "                dw = Tensor(N, dim_out,dim_in)    # dL/dw\n",
    "                ds = Tensor(N, dim_out)           # dL/ds\n",
    "                dx = Tensor(N, dim_out)           # dL/dx\n",
    "            \"\"\"\n",
    "            \n",
    "            # dl/dx : size = N,d_out\n",
    "            if layer == self.n_layer:\n",
    "                self.backward_list[layer].dx = dloss(x,Y)   \n",
    "            else:\n",
    "                self.backward_list[layer].dx = self.backward_list[layer+1].ds.mm(self.param_list[layer].w)\n",
    "            \"\"\"\n",
    "            hint:\n",
    "                dloss(x,Y)                 = (N, d_out_{last_layer})\n",
    "                backward_list[layer+1].ds  = (N, d_out_{layer+1})\n",
    "                param_list[layer].w        = (d_out_{layer+1}, d_in_{layer+1}) = (d_out_{layer+1}, d_out_{layer}) \n",
    "            \"\"\"\n",
    "                \n",
    "            #dl/ds : size = N,d_out\n",
    "            self.backward_list[layer].ds = self.backward_list[layer].dx * dActivation(self.act_list[layer-1], self.forward_list[layer].s ) \n",
    "            \"\"\"\n",
    "            hint:\n",
    "                backward_list[layer].dx  = (N, d_out)\n",
    "                self.act_list[layer-1]   : activation type of layer = layer (0,1,2 or 3)\n",
    "                forward_list[layer].s    = (N, d_out)\n",
    "                dActivation (code, s )   = (N, d_out)\n",
    "            \"\"\"\n",
    "            \n",
    "            #dl/dw : size = (N, d_out, d_in)\n",
    "            ds_unsq = self.backward_list[layer].ds.unsqueeze(1)    \n",
    "            x_unsq  = self.forward_list[layer-1].x.unsqueeze(2)\n",
    "            self.backward_list[layer].dw = ds_unsq * x_unsq        # enjoying broadcasting magic\n",
    "            \"\"\"\n",
    "            hint:\n",
    "                backward_list[layer].ds        = (N, d_out)\n",
    "                self.forward_list[layer-1].x   = (N, d_in)\n",
    "                ds_unsq                        = (N, 1, d_out)\n",
    "                dx_unsq                        = (N, d_in, 1)\n",
    "                backward_list[layer].dw        = (N, d_out, d_in)\n",
    "            \"\"\"\n",
    "            \n",
    "            #dl/db : size = (N, d_out)\n",
    "            self.backward_list[layer].db = self.backward_list[layer].ds\n",
    "            \n",
    "            \"\"\"\n",
    "            hint:\n",
    "                backward_list[layer].ds        = (N, d_out)\n",
    "                backward_list[layer].db        = (N, d_out)\n",
    "            \"\"\"\n",
    "           \n",
    "        # summing all batch grads\n",
    "        for layer in list (range(0,self.n_layer)):\n",
    "            self.param_list[layer].db = self.backward_list[layer+1].db.sum(0).unsqueeze(1)/self.N  \n",
    "            self.param_list[layer].dw = self.backward_list[layer+1].dw.sum(0)/self.N   \n",
    "        \n",
    "        # evaluating loss: Not crucial \n",
    "        L = loss(x,Y)   \n",
    "        return L\n",
    "        \n",
    "    \n",
    "    def parameteres (self):\n",
    "        return self.param_list\n",
    "        \n",
    "        \"\"\"\n",
    "    def update(self,eta):\n",
    "        for p in self.param_list:\n",
    "            #print (p.data)\n",
    "            #print (p.grad())\n",
    "            #delta =  eta* p.grad\n",
    "            #p.data = p.data+ 5\n",
    "            #print (p.data)\n",
    "            p.data = eta* p.grad + p.data\n",
    "    \"\"\"\n",
    "    \n",
    "    def train(self,X,Y,eta):\n",
    "        for i in list(range(10000)):\n",
    "            L=self.backward(X,Y)\n",
    "            # print(i, L)\n",
    "            for p in self.parameteres():\n",
    "                p.b -= eta* p.db\n",
    "                p.w -= eta* p.dw\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notes for Ehsan\n",
    "\n",
    "Backward now doesn't return anything! but you can access the parametes and the grads of each layer using the following\n",
    "    \n",
    "    model.parameteres\n",
    "\n",
    "This returns a list with length = nb_of_layers, starting from 0 and ending at L-1. Here is simple example, using our usual hand-made batch.\n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = Tensor(7).normal_()\n",
    "X = torch.cat((x.view(1,-1),x.view(1,-1)*3,x.view(1,-1)*x.view(1,-1)),0)\n",
    "Y= Tensor(3,10).normal_();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq = [Linear(7,2),'lin',Linear(2,5),'relu',Linear(5,10),'lin']    \n",
    "model = Net(seq,X,Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12.801758687157417"
      ]
     },
     "execution_count": 355,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.backward(X,Y)         #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-1.7008  0.6946  0.0000\n",
      " 0.7559  1.7884  0.0000\n",
      "-0.8335 -1.0073  0.0000\n",
      "-1.2169  0.6870  0.0000\n",
      "-0.9247  0.1177  0.0000\n",
      "[torch.FloatTensor of size 5x3]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\tensor.py:318: UserWarning: other is not broadcastable to self, but they have the same number of elements.  Falling back to deprecated pointwise behavior.\n",
      "  return self.sub_(other)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " -1.6715  -4.4737  -0.1290\n",
      "  0.5314   2.7045  -4.2287\n",
      "  0.9971  25.6167   0.2908\n",
      " -2.3456   0.5917  -1.0771\n",
      "  0.9425   0.3621  -1.8605\n",
      "[torch.FloatTensor of size 5x3]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(model.parameteres()[1].data())\n",
    "model.train(X,Y,0.01)\n",
    "print(model.parameteres()[1].data())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
