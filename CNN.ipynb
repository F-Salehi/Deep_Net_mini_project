{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dlc_bci as bci\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "from random import randint\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from torch import Tensor\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://documents.epfl.ch/users/f/fl/fleuret/www/data/bci/sp1s_aa_test.txt\n",
      "Downloading https://documents.epfl.ch/users/f/fl/fleuret/www/data/bci/labels_data_set_iv.txt\n",
      "('train_input size:', torch.Size([316, 28, 1, 50]))\n",
      "('train_target size:', torch.Size([316]))\n",
      "('test_input size:', torch.Size([100, 28, 1, 50]))\n",
      "('test_target size:', torch.Size([100]))\n"
     ]
    }
   ],
   "source": [
    "train_input , train_target = bci.load ( root = \"./ data_bci\")\n",
    "test_input , test_target = bci.load ( root = \"./ data_bci \" , train = False )\n",
    "\n",
    "train_target = Variable(train_target)\n",
    "train_input = Variable(train_input.unsqueeze(dim=2))\n",
    "test_input = Variable(test_input.unsqueeze(dim=2))\n",
    "test_target = Variable(test_target)\n",
    "\n",
    "mu, std = train_input.data.mean(), train_input.data.std() \n",
    "train_input.data.sub_(mu).div_(std)\n",
    "test_input.data.sub_(mu).div_(std)\n",
    "\n",
    "\n",
    "print('train_input size:' , train_input.size())\n",
    "print ( 'train_target size:' , train_target.size())\n",
    "print ('test_input size:' , test_input.size())\n",
    "print ('test_target size:' , test_target.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset:\n",
    "    def __init__(self,train_data,train_target):\n",
    "        self.train_data = train_data.data\n",
    "        self.train_target = train_target\n",
    "    def __getitem__(self, index):\n",
    "        return self.train_data[index], self.train_target[index]\n",
    "    def __len__(self):\n",
    "        return len(self.train_target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolutional neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net,self).__init__()\n",
    "        self.conv1 = nn.Conv2d (28, 32, kernel_size = (1,5))    # after conv: 20*32*1*46\n",
    "        self.avg_pool1 = nn.AvgPool2d(kernel_size = (1,4))      # size -> 1/4 *size\n",
    "        self.conv2 = nn.Conv2d (32 , 64, kernel_size = (1,5))   \n",
    "        self.fc1 = nn.Linear (448 , 50)                         # size : 64 ->32 => 448/2\n",
    "        self.dr = nn.Dropout(p=0.8)\n",
    "        self.fc2 = nn.Linear (50 , 2)\n",
    "        \n",
    "    def forward (self , x):\n",
    "        x = self.conv1(x)\n",
    "        #print ('after first conv',x.size())\n",
    "        x = F.relu(self.avg_pool1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = x.view (-1, 448)\n",
    "        x = self.dr(x)\n",
    "        x = F.relu ( self.fc1 (x))\n",
    "        x = self.dr(x)\n",
    "        x = self.fc2(x)\n",
    "                   #x = F.relu(F.max_pool2d(self.conv1(x), kernel_size = 3, stride =3) )\n",
    "        #x = F.relu (F.max_pool2d ( self.conv2(x), kernel_size =2, stride =2) )\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The function computes the accuracy\n",
    "def accuracy(output,target):\n",
    "    return (output.float()==target.float()).float().sum()/len(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = Dataset(train_input,train_target)\n",
    "dataset_loader = torch.utils.data.DataLoader(data,\n",
    "                                             batch_size = 40, shuffle=True,\n",
    "                                             num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, 'loss train:', 0.6936902403831482, 'loss test:', 0.6947332620620728, ' accuracy test:', 0.44999998807907104)\n",
      "(1, 'loss train:', 0.7020915746688843, 'loss test:', 0.6980192065238953, ' accuracy test:', 0.47999998927116394)\n",
      "(2, 'loss train:', 0.69966721534729, 'loss test:', 0.6979005336761475, ' accuracy test:', 0.5099999904632568)\n",
      "(3, 'loss train:', 0.6976155042648315, 'loss test:', 0.7213453054428101, ' accuracy test:', 0.5)\n",
      "(4, 'loss train:', 0.6964417099952698, 'loss test:', 0.6897997260093689, ' accuracy test:', 0.5600000023841858)\n",
      "(5, 'loss train:', 0.6948499083518982, 'loss test:', 0.7155714631080627, ' accuracy test:', 0.4000000059604645)\n",
      "(6, 'loss train:', 0.6868491172790527, 'loss test:', 0.6975237131118774, ' accuracy test:', 0.4699999988079071)\n",
      "(7, 'loss train:', 0.6775312423706055, 'loss test:', 0.707350492477417, ' accuracy test:', 0.44999998807907104)\n",
      "(8, 'loss train:', 0.6778134107589722, 'loss test:', 0.6980429291725159, ' accuracy test:', 0.5299999713897705)\n",
      "(9, 'loss train:', 0.6808695197105408, 'loss test:', 0.7082602977752686, ' accuracy test:', 0.5400000214576721)\n",
      "(10, 'loss train:', 0.678126335144043, 'loss test:', 0.6840741634368896, ' accuracy test:', 0.5600000023841858)\n",
      "(11, 'loss train:', 0.6708483099937439, 'loss test:', 0.711743175983429, ' accuracy test:', 0.5)\n",
      "(12, 'loss train:', 0.6257956027984619, 'loss test:', 0.6654869318008423, ' accuracy test:', 0.6299999952316284)\n",
      "(13, 'loss train:', 0.6224875450134277, 'loss test:', 0.6789970993995667, ' accuracy test:', 0.6100000143051147)\n",
      "(14, 'loss train:', 0.5810245275497437, 'loss test:', 0.7043200731277466, ' accuracy test:', 0.6000000238418579)\n",
      "(15, 'loss train:', 0.6047444343566895, 'loss test:', 0.6909934878349304, ' accuracy test:', 0.5400000214576721)\n",
      "(16, 'loss train:', 0.5916317701339722, 'loss test:', 0.6931590437889099, ' accuracy test:', 0.6000000238418579)\n",
      "(17, 'loss train:', 0.5835136771202087, 'loss test:', 0.6176123023033142, ' accuracy test:', 0.699999988079071)\n",
      "(18, 'loss train:', 0.5309669375419617, 'loss test:', 0.6340895295143127, ' accuracy test:', 0.6499999761581421)\n",
      "(19, 'loss train:', 0.5253915190696716, 'loss test:', 0.6834073066711426, ' accuracy test:', 0.6000000238418579)\n",
      "(20, 'loss train:', 0.4846426546573639, 'loss test:', 0.6053994297981262, ' accuracy test:', 0.7099999785423279)\n",
      "(21, 'loss train:', 0.4398854076862335, 'loss test:', 0.589501142501831, ' accuracy test:', 0.6700000166893005)\n",
      "(22, 'loss train:', 0.5087209939956665, 'loss test:', 0.575935959815979, ' accuracy test:', 0.7200000286102295)\n",
      "(23, 'loss train:', 0.4963388442993164, 'loss test:', 0.5971720814704895, ' accuracy test:', 0.7200000286102295)\n",
      "(24, 'loss train:', 0.44244468212127686, 'loss test:', 0.6276927590370178, ' accuracy test:', 0.6600000262260437)\n",
      "(25, 'loss train:', 0.38740307092666626, 'loss test:', 0.6595918536186218, ' accuracy test:', 0.699999988079071)\n",
      "(26, 'loss train:', 0.4754132032394409, 'loss test:', 0.7076838612556458, ' accuracy test:', 0.6399999856948853)\n",
      "(27, 'loss train:', 0.4149358868598938, 'loss test:', 0.6428096294403076, ' accuracy test:', 0.7400000095367432)\n",
      "(28, 'loss train:', 0.35567256808280945, 'loss test:', 0.6335616111755371, ' accuracy test:', 0.6800000071525574)\n",
      "(29, 'loss train:', 0.33012962341308594, 'loss test:', 0.6025680899620056, ' accuracy test:', 0.6700000166893005)\n",
      "(30, 'loss train:', 0.38454264402389526, 'loss test:', 0.6689946055412292, ' accuracy test:', 0.7200000286102295)\n",
      "(31, 'loss train:', 0.4375242292881012, 'loss test:', 0.7814648151397705, ' accuracy test:', 0.7200000286102295)\n",
      "(32, 'loss train:', 0.58051598072052, 'loss test:', 0.7095955014228821, ' accuracy test:', 0.6899999976158142)\n",
      "(33, 'loss train:', 0.3527197539806366, 'loss test:', 0.7053852081298828, ' accuracy test:', 0.7200000286102295)\n",
      "(34, 'loss train:', 0.30287617444992065, 'loss test:', 0.7018148899078369, ' accuracy test:', 0.7200000286102295)\n",
      "(35, 'loss train:', 0.3639957010746002, 'loss test:', 0.8325339555740356, ' accuracy test:', 0.7200000286102295)\n",
      "(36, 'loss train:', 0.38045990467071533, 'loss test:', 0.8485958576202393, ' accuracy test:', 0.699999988079071)\n",
      "(37, 'loss train:', 0.29398640990257263, 'loss test:', 0.897174596786499, ' accuracy test:', 0.699999988079071)\n",
      "(38, 'loss train:', 0.25178128480911255, 'loss test:', 0.7180196642875671, ' accuracy test:', 0.7300000190734863)\n",
      "(39, 'loss train:', 0.2165313959121704, 'loss test:', 0.7510665655136108, ' accuracy test:', 0.7300000190734863)\n",
      "(40, 'loss train:', 0.22039048373699188, 'loss test:', 0.7892088294029236, ' accuracy test:', 0.7400000095367432)\n",
      "(41, 'loss train:', 0.25402259826660156, 'loss test:', 0.9793621301651001, ' accuracy test:', 0.7400000095367432)\n",
      "(42, 'loss train:', 0.24707797169685364, 'loss test:', 1.0493923425674438, ' accuracy test:', 0.7300000190734863)\n",
      "(43, 'loss train:', 0.22350721061229706, 'loss test:', 0.9594540596008301, ' accuracy test:', 0.7099999785423279)\n",
      "(44, 'loss train:', 0.23150081932544708, 'loss test:', 0.7678042054176331, ' accuracy test:', 0.75)\n",
      "(45, 'loss train:', 0.20763693749904633, 'loss test:', 0.9555362462997437, ' accuracy test:', 0.7200000286102295)\n",
      "(46, 'loss train:', 0.2504507303237915, 'loss test:', 0.7190233469009399, ' accuracy test:', 0.6800000071525574)\n",
      "(47, 'loss train:', 0.3407554030418396, 'loss test:', 0.6056627631187439, ' accuracy test:', 0.7300000190734863)\n",
      "(48, 'loss train:', 0.20586594939231873, 'loss test:', 1.2284339666366577, ' accuracy test:', 0.6600000262260437)\n",
      "(49, 'loss train:', 0.165985107421875, 'loss test:', 1.1460965871810913, ' accuracy test:', 0.6899999976158142)\n",
      "(50, 'loss train:', 0.17505109310150146, 'loss test:', 1.389999508857727, ' accuracy test:', 0.6800000071525574)\n",
      "(51, 'loss train:', 0.2230251133441925, 'loss test:', 0.9332113862037659, ' accuracy test:', 0.7300000190734863)\n",
      "(52, 'loss train:', 0.23028606176376343, 'loss test:', 0.756264328956604, ' accuracy test:', 0.7099999785423279)\n",
      "(53, 'loss train:', 0.16773150861263275, 'loss test:', 1.207647681236267, ' accuracy test:', 0.6800000071525574)\n",
      "(54, 'loss train:', 0.21324694156646729, 'loss test:', 1.0369056463241577, ' accuracy test:', 0.6700000166893005)\n",
      "(55, 'loss train:', 0.18790879845619202, 'loss test:', 1.1042141914367676, ' accuracy test:', 0.6299999952316284)\n",
      "(56, 'loss train:', 0.1863052099943161, 'loss test:', 1.1536221504211426, ' accuracy test:', 0.7300000190734863)\n",
      "(57, 'loss train:', 0.17846685647964478, 'loss test:', 1.3864551782608032, ' accuracy test:', 0.6600000262260437)\n",
      "(58, 'loss train:', 0.1841728389263153, 'loss test:', 1.1314308643341064, ' accuracy test:', 0.7099999785423279)\n",
      "(59, 'loss train:', 0.18482539057731628, 'loss test:', 1.2339609861373901, ' accuracy test:', 0.6899999976158142)\n",
      "(60, 'loss train:', 0.1413055807352066, 'loss test:', 1.4362473487854004, ' accuracy test:', 0.699999988079071)\n",
      "(61, 'loss train:', 0.1329931914806366, 'loss test:', 1.4211992025375366, ' accuracy test:', 0.699999988079071)\n",
      "(62, 'loss train:', 0.13104741275310516, 'loss test:', 1.4706438779830933, ' accuracy test:', 0.7099999785423279)\n",
      "(63, 'loss train:', 0.17759446799755096, 'loss test:', 1.6002906560897827, ' accuracy test:', 0.6700000166893005)\n",
      "(64, 'loss train:', 0.14057444036006927, 'loss test:', 1.4577276706695557, ' accuracy test:', 0.699999988079071)\n",
      "(65, 'loss train:', 0.17055250704288483, 'loss test:', 1.4439946413040161, ' accuracy test:', 0.7099999785423279)\n",
      "(66, 'loss train:', 0.09865336865186691, 'loss test:', 1.6243007183074951, ' accuracy test:', 0.6800000071525574)\n",
      "(67, 'loss train:', 0.22459621727466583, 'loss test:', 1.5299409627914429, ' accuracy test:', 0.7099999785423279)\n",
      "(68, 'loss train:', 0.17302702367305756, 'loss test:', 1.1548938751220703, ' accuracy test:', 0.6899999976158142)\n",
      "(69, 'loss train:', 0.1031130775809288, 'loss test:', 2.118759870529175, ' accuracy test:', 0.699999988079071)\n",
      "(70, 'loss train:', 0.10206512361764908, 'loss test:', 1.061660885810852, ' accuracy test:', 0.7400000095367432)\n",
      "(71, 'loss train:', 0.13942939043045044, 'loss test:', 1.282608151435852, ' accuracy test:', 0.7200000286102295)\n",
      "(72, 'loss train:', 0.05999775230884552, 'loss test:', 1.5047810077667236, ' accuracy test:', 0.7300000190734863)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(73, 'loss train:', 0.1832265704870224, 'loss test:', 1.0660862922668457, ' accuracy test:', 0.7900000214576721)\n",
      "(74, 'loss train:', 0.15368202328681946, 'loss test:', 1.238193392753601, ' accuracy test:', 0.7099999785423279)\n",
      "(75, 'loss train:', 0.16754645109176636, 'loss test:', 1.2689553499221802, ' accuracy test:', 0.7300000190734863)\n",
      "(76, 'loss train:', 0.37511715292930603, 'loss test:', 2.415447473526001, ' accuracy test:', 0.699999988079071)\n",
      "(77, 'loss train:', 0.12390346825122833, 'loss test:', 1.5053437948226929, ' accuracy test:', 0.699999988079071)\n",
      "(78, 'loss train:', 0.17766223847866058, 'loss test:', 0.8611512184143066, ' accuracy test:', 0.6800000071525574)\n",
      "(79, 'loss train:', 0.07056857645511627, 'loss test:', 1.5393235683441162, ' accuracy test:', 0.7400000095367432)\n",
      "(80, 'loss train:', 0.11096471548080444, 'loss test:', 1.3536381721496582, ' accuracy test:', 0.699999988079071)\n",
      "(81, 'loss train:', 0.19003765285015106, 'loss test:', 1.4711185693740845, ' accuracy test:', 0.6600000262260437)\n",
      "(82, 'loss train:', 0.07230162620544434, 'loss test:', 2.0247340202331543, ' accuracy test:', 0.7699999809265137)\n",
      "(83, 'loss train:', 0.09602496773004532, 'loss test:', 1.8525111675262451, ' accuracy test:', 0.6899999976158142)\n",
      "(84, 'loss train:', 0.10433261841535568, 'loss test:', 2.3671059608459473, ' accuracy test:', 0.7300000190734863)\n",
      "(85, 'loss train:', 0.08939288556575775, 'loss test:', 1.7770347595214844, ' accuracy test:', 0.6600000262260437)\n",
      "(86, 'loss train:', 0.06582548469305038, 'loss test:', 1.4478679895401, ' accuracy test:', 0.7200000286102295)\n",
      "(87, 'loss train:', 0.09628766775131226, 'loss test:', 1.604054570198059, ' accuracy test:', 0.7599999904632568)\n",
      "(88, 'loss train:', 0.10010381042957306, 'loss test:', 2.2433035373687744, ' accuracy test:', 0.699999988079071)\n",
      "(89, 'loss train:', 0.1223411113023758, 'loss test:', 1.3037563562393188, ' accuracy test:', 0.75)\n",
      "(90, 'loss train:', 0.11825372278690338, 'loss test:', 1.9052637815475464, ' accuracy test:', 0.6600000262260437)\n",
      "(91, 'loss train:', 0.06488744914531708, 'loss test:', 1.723130226135254, ' accuracy test:', 0.699999988079071)\n",
      "(92, 'loss train:', 0.06376643478870392, 'loss test:', 1.7076759338378906, ' accuracy test:', 0.7200000286102295)\n",
      "(93, 'loss train:', 0.04837143048644066, 'loss test:', 1.6374889612197876, ' accuracy test:', 0.699999988079071)\n",
      "(94, 'loss train:', 0.11947345733642578, 'loss test:', 2.0183653831481934, ' accuracy test:', 0.7400000095367432)\n",
      "(95, 'loss train:', 0.0891098901629448, 'loss test:', 1.3750770092010498, ' accuracy test:', 0.6800000071525574)\n",
      "(96, 'loss train:', 0.5085880160331726, 'loss test:', 2.491562843322754, ' accuracy test:', 0.7200000286102295)\n",
      "(97, 'loss train:', 0.16002899408340454, 'loss test:', 2.4437901973724365, ' accuracy test:', 0.7099999785423279)\n",
      "(98, 'loss train:', 0.05011959373950958, 'loss test:', 2.146343946456909, ' accuracy test:', 0.7200000286102295)\n",
      "(99, 'loss train:', 0.0695275217294693, 'loss test:', 2.8967275619506836, ' accuracy test:', 0.7300000190734863)\n"
     ]
    }
   ],
   "source": [
    "### train the model\n",
    "net = Net()\n",
    "epoch = 1000\n",
    "batch_size = 20\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(),lr = 0.01,momentum = 0.5)\n",
    "\n",
    "\n",
    "\n",
    "i = 0\n",
    "for e in range(epoch):\n",
    "    for b in range(0,train_input.size(0),batch_size):\n",
    "        input_ = train_input.narrow(0,b,min(batch_size,train_input.size(0)-b))\n",
    "        target = train_target.narrow(0,b,min(batch_size,train_input.size(0)-b))\n",
    "        output = net(input_).view(min(batch_size,train_input.size(0)-b),2)\n",
    "        loss = criterion(output,target)\n",
    "        net.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    if e%10 == 0: # print the loss every 10 epochs\n",
    "        output = net(test_input).view(-1,2)\n",
    "        a,predicted_class = output.max(dim=1)\n",
    "        output_train = net(train_input).view(-1,2)\n",
    "        a,predicted_class_train = output.max(dim=1)\n",
    "        \n",
    "        #predicted_class = 1*(predicted_class.float().mean(dim=1) > 0.5)\n",
    "        print(e//10,'loss train:',criterion(output_train,train_target).data[0],'loss test:',criterion(output,test_target).data[0],' accuracy test:',accuracy(predicted_class,test_target).data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Augmentaiotn: Averaging two datapoints in the same class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "class0 = train_input[(train_target == 0).nonzero(),]\n",
    "class1 = train_input[(train_target == 1).nonzero(),]\n",
    "class0.squeeze_(dim=1)\n",
    "class1.squeeze_(dim=1)\n",
    "N = class0.shape[0]\n",
    "train_input_aug0 = [(class0[i]+class0[j])/2 for i in range(N) for j in range(i,N)]\n",
    "train_input_aug0 = torch.stack(train_input_aug0)\n",
    "train_out_aug0 = Variable(Tensor(train_input_aug0.shape[0]).fill_(0))\n",
    "N = class1.shape[0]\n",
    "train_input_aug1 = [(class1[i]+class1[j])/2 for i in range(N) for j in range(i,N)]\n",
    "train_input_aug1 = torch.stack(train_input_aug1)\n",
    "train_out_aug1 = Variable(Tensor(train_input_aug1.shape[0]).fill_(1))\n",
    "\n",
    "###\n",
    "train_aug = torch.cat((train_input_aug0,train_input_aug1))\n",
    "train_target_aug = torch.cat((train_out_aug0,train_out_aug1)).data.long()\n",
    "\n",
    "mu, std = train_aug.data.mean(), train_aug.data.std() ;\n",
    "train_aug.data.sub_(mu).div_(std);\n",
    "test_input.data.sub_(mu).div_(std);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.autograd.variable.Variable"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(train_aug)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = Dataset(train_aug,train_target_aug)\n",
    "dataset_loader = torch.utils.data.DataLoader(data,\n",
    "                                             batch_size = 40, shuffle=True,\n",
    "                                             num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "net = Net()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, 'loss train:', 0.6843971610069275, 'loss test:', 0.7019619941711426, ' accuracy test:', 0.5099999904632568)\n",
      "(1, 'loss train:', 0.6991146802902222, 'loss test:', 0.7469719052314758, ' accuracy test:', 0.5699999928474426)\n",
      "(2, 'loss train:', 0.9319414496421814, 'loss test:', 1.0630385875701904, ' accuracy test:', 0.5799999833106995)\n",
      "(3, 'loss train:', 0.5540997385978699, 'loss test:', 0.7864527106285095, ' accuracy test:', 0.6700000166893005)\n",
      "(4, 'loss train:', 0.43757498264312744, 'loss test:', 0.7659677267074585, ' accuracy test:', 0.75)\n",
      "(5, 'loss train:', 0.5060749650001526, 'loss test:', 0.9224059581756592, ' accuracy test:', 0.7099999785423279)\n",
      "(6, 'loss train:', 0.410137802362442, 'loss test:', 0.741931676864624, ' accuracy test:', 0.7900000214576721)\n",
      "(7, 'loss train:', 0.31868189573287964, 'loss test:', 0.9916681051254272, ' accuracy test:', 0.75)\n",
      "(8, 'loss train:', 0.32711607217788696, 'loss test:', 0.8552981615066528, ' accuracy test:', 0.7799999713897705)\n",
      "(9, 'loss train:', 0.30806076526641846, 'loss test:', 0.8151366710662842, ' accuracy test:', 0.7300000190734863)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "### train the model\n",
    "epoch = 10\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(),lr = 0.01,momentum = 0.5)\n",
    "i = 0\n",
    "for e in range(epoch):\n",
    "    for input_, target_ in dataset_loader:\n",
    "        input_ = Variable(input_)\n",
    "        target_ = Variable(target_)\n",
    "        output = net(input_).view(-1,2)\n",
    "        loss = criterion(output,target_) \n",
    "        net.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    if e%1 == 0: # print the loss every 10 epochs\n",
    "        output = net(test_input).view(-1,2)\n",
    "        a,predicted_class = output.max(dim=1)\n",
    "        output_train = net(train_input).view(-1,2)\n",
    "        a,predicted_class_train = output.max(dim=1)\n",
    "        #predicted_class = 1*(predicted_class.float().mean(dim=1) > 0.5)\n",
    "        print(e//1,'loss train:',criterion(output_train,train_target).data[0],'loss test:',criterion(output,test_target).data[0],' accuracy test:',accuracy(predicted_class,test_target).data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding L1 reguralizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def l1_reguralizer(paramters):\n",
    "    loss = 0\n",
    "    for p in paramters:\n",
    "        loss += torch.abs(p).mean()\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = Net()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, 'loss train:', 0.69, 'accuracy train:', 0.56, 'loss test:', 0.72, ' accuracy test:', 0.49)\n",
      "(1, 'loss train:', 0.68, 'accuracy train:', 0.63, 'loss test:', 0.87, ' accuracy test:', 0.55)\n",
      "(2, 'loss train:', 0.55, 'accuracy train:', 0.73, 'loss test:', 0.75, ' accuracy test:', 0.73)\n",
      "(3, 'loss train:', 0.52, 'accuracy train:', 0.76, 'loss test:', 0.9, ' accuracy test:', 0.7)\n",
      "(4, 'loss train:', 0.43, 'accuracy train:', 0.81, 'loss test:', 0.76, ' accuracy test:', 0.71)\n",
      "(5, 'loss train:', 0.41, 'accuracy train:', 0.84, 'loss test:', 1.03, ' accuracy test:', 0.72)\n",
      "(6, 'loss train:', 0.38, 'accuracy train:', 0.82, 'loss test:', 0.97, ' accuracy test:', 0.71)\n",
      "(7, 'loss train:', 0.37, 'accuracy train:', 0.84, 'loss test:', 1.13, ' accuracy test:', 0.7)\n",
      "(8, 'loss train:', 1.4, 'accuracy train:', 0.7, 'loss test:', 2.79, ' accuracy test:', 0.56)\n",
      "(9, 'loss train:', 0.69, 'accuracy train:', 0.81, 'loss test:', 1.39, ' accuracy test:', 0.78)\n",
      "(10, 'loss train:', 0.28, 'accuracy train:', 0.89, 'loss test:', 0.83, ' accuracy test:', 0.77)\n",
      "(11, 'loss train:', 0.33, 'accuracy train:', 0.87, 'loss test:', 1.47, ' accuracy test:', 0.77)\n",
      "(12, 'loss train:', 0.23, 'accuracy train:', 0.91, 'loss test:', 0.83, ' accuracy test:', 0.81)\n",
      "(13, 'loss train:', 0.26, 'accuracy train:', 0.9, 'loss test:', 1.07, ' accuracy test:', 0.8)\n",
      "(14, 'loss train:', 0.19, 'accuracy train:', 0.94, 'loss test:', 1.64, ' accuracy test:', 0.72)\n",
      "(15, 'loss train:', 0.21, 'accuracy train:', 0.92, 'loss test:', 1.28, ' accuracy test:', 0.79)\n",
      "(16, 'loss train:', 0.4, 'accuracy train:', 0.86, 'loss test:', 1.44, ' accuracy test:', 0.74)\n",
      "(17, 'loss train:', 0.27, 'accuracy train:', 0.9, 'loss test:', 1.27, ' accuracy test:', 0.74)\n",
      "(18, 'loss train:', 0.31, 'accuracy train:', 0.9, 'loss test:', 1.91, ' accuracy test:', 0.76)\n",
      "(19, 'loss train:', 0.25, 'accuracy train:', 0.92, 'loss test:', 1.66, ' accuracy test:', 0.79)\n",
      "(20, 'loss train:', 0.31, 'accuracy train:', 0.88, 'loss test:', 0.92, ' accuracy test:', 0.74)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "### train the model\n",
    "epoch = 100\n",
    "lambda_ = 10**-2\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(),lr = 0.01,momentum = 0.5)\n",
    "i = 0\n",
    "loss_train = []\n",
    "loss_test = []\n",
    "accuracy_train = []\n",
    "accuracy_test = []\n",
    "for e in range(epoch):\n",
    "    if e == 30:\n",
    "        optimizer = optim.SGD(net.parameters(),lr = 0.003,momentum = 0.5)\n",
    "    if e== 60:\n",
    "        optimizer = optim.SGD(net.parameters(),lr = 0.001,momentum = 0.5)\n",
    "    for input_, target_ in dataset_loader:\n",
    "        input_ = Variable(input_)\n",
    "        target_ = Variable(target_)\n",
    "        output = net(input_).view(-1,2)\n",
    "        loss = criterion(output,target_) + lambda_ * l1_reguralizer(net.parameters())\n",
    "        net.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        \n",
    "    if e%1 == 0: # print the loss every 10 epochs\n",
    "        output = net(test_input).view(-1,2)\n",
    "        a,predicted_class = output.max(dim=1)\n",
    "        output_train = net(train_input).view(-1,2)\n",
    "        a,predicted_class_train = output_train.max(dim=1)\n",
    "        accuracy_train.append(accuracy(predicted_class_train,train_target).data[0])\n",
    "        accuracy_test.append(accuracy(predicted_class,test_target).data[0])\n",
    "        loss_train.append(criterion(output_train,train_target).data[0]+ lambda_ * l1_reguralizer(net.parameters()).data[0])\n",
    "        loss_test.append(criterion(output,test_target).data[0]+ lambda_ * l1_reguralizer(net.parameters()).data[0])\n",
    "        print(e//1,'loss train:',round(loss_train[-1],2), 'accuracy train:', round(accuracy_train[-1],2),\\\n",
    "              'loss test:',round(loss_test[-1],2),' accuracy test:',round(accuracy_test[-1],2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.plot(loss_train)\n",
    "plt.plot(loss_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(accuracy_train)\n",
    "plt.plot(accuracy_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Average between 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class0 = train_input[(train_target == 0).nonzero(),]\n",
    "class1 = train_input[(train_target == 1).nonzero(),]\n",
    "class0.squeeze_(dim=1)\n",
    "class1.squeeze_(dim=1)\n",
    "\n",
    "N = class0.shape[0]\n",
    "train_input_aug0 = [(class0[randint(0,N-1)]+class0[randint(0,N-1)]+ class0[randint(0,N-1)])/3 for i in range(200*10**3)]\n",
    "train_input_aug0 = torch.stack(train_input_aug0)\n",
    "train_out_aug0 = Variable(Tensor(train_input_aug0.shape[0]).fill_(0))\n",
    "N = class1.shape[0]\n",
    "train_input_aug1 = [(class0[randint(0,N-1)]+class0[randint(0,N-1)]+ class0[randint(0,N-1)])/3 for i in range(200*10**3)]\n",
    "train_input_aug1 = torch.stack(train_input_aug1)\n",
    "train_out_aug1 = Variable(Tensor(train_input_aug1.shape[0]).fill_(1))\n",
    "\n",
    "###\n",
    "train_aug = torch.cat((train_input_aug0,train_input_aug1))\n",
    "train_target_aug = torch.cat((train_out_aug0,train_out_aug1)).data.long()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = Dataset(train_aug,train_target_aug)\n",
    "dataset_loader = torch.utils.data.DataLoader(data,\n",
    "                                             batch_size = 100, shuffle=True,\n",
    "                                             num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = Net()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### train the model\n",
    "epoch = 10\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(),lr = 0.0001,momentum = 0.5)\n",
    "i = 0\n",
    "for e in range(epoch):\n",
    "    for input_, target_ in dataset_loader:\n",
    "        input_ = Variable(input_)\n",
    "        target_ = Variable(target_)\n",
    "        output = net(input_).view(-1,2)\n",
    "        loss = criterion(output,target_)\n",
    "        net.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    if e%1 == 0: # print the loss every 10 epochs\n",
    "        output = net(test_input).view(-1,2)\n",
    "        a,predicted_class = output.max(dim=1)\n",
    "        output_train = net(train_input).view(-1,2)\n",
    "        a,predicted_class_train = output.max(dim=1)\n",
    "        #predicted_class = 1*(predicted_class.float().mean(dim=1) > 0.5)\n",
    "        print(e//1,'loss train:',criterion(output_train,train_target).data[0],'loss test:',criterion(output,test_target).data[0],' accuracy test:',accuracy(predicted_class,test_target).data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
