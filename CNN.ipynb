{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import dlc_bci as bci\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from torch import Tensor\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_input size: torch.Size([316, 28, 1, 50])\n",
      "train_target size: torch.Size([316])\n",
      "test_input size: torch.Size([100, 28, 1, 50])\n",
      "test_target size: torch.Size([100])\n"
     ]
    }
   ],
   "source": [
    "train_input , train_target = bci.load ( root = \"./ data_bci\")\n",
    "test_input , test_target = bci.load ( root = \"./ data_bci \" , train = False )\n",
    "\n",
    "train_target = Variable(train_target)\n",
    "train_input = Variable(train_input.unsqueeze(dim=2))\n",
    "test_input = Variable(test_input.unsqueeze(dim=2))\n",
    "test_target = Variable(test_target)\n",
    "\n",
    "mu, std = train_input.data.mean(), train_input.data.std() \n",
    "train_input.data.sub_(mu).div_(std)\n",
    "test_input.data.sub_(mu).div_(std)\n",
    "\n",
    "\n",
    "print('train_input size:' , train_input.size())\n",
    "print ( 'train_target size:' , train_target.size())\n",
    "print ('test_input size:' , test_input.size())\n",
    "print ('test_target size:' , test_target.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# The function computes the accuracy\n",
    "def accuracy(output,target):\n",
    "    return (output.float()==target.float()).float().sum()/len(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolutional neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dim0_conv1=28\n",
    "dim1_conv1=32\n",
    "dim0_conv2=dim1_conv1\n",
    "dim1_conv2= 40\n",
    "\n",
    "fc1_dim0 =  1\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net,self).__init__()\n",
    "        self.conv1 = nn.Conv2d (28, 32, kernel_size = (1,5))    # after conv: 20*32*1*46\n",
    "        self.avg_pool1 = nn.AvgPool2d(kernel_size = (1,4))      # size -> 1/4 *size\n",
    "        self.conv2 = nn.Conv2d (32 , 64, kernel_size = (1,5))   \n",
    "        self.fc1 = nn.Linear (448 , 50)                         # size : 64 ->32 => 448/2\n",
    "        self.dr = nn.Dropout(p=0.8)\n",
    "        self.fc2 = nn.Linear (50 , 2)\n",
    "        \n",
    "    def forward (self , x):\n",
    "        x = self.conv1(x)\n",
    "        #print ('after first conv',x.size())\n",
    "        x = F.relu(self.avg_pool1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = x.view (-1, 448)\n",
    "        x = self.dr(x)\n",
    "        x = F.relu ( self.fc1 (x))\n",
    "        x = self.fc2(x)\n",
    "                   #x = F.relu(F.max_pool2d(self.conv1(x), kernel_size = 3, stride =3) )\n",
    "        #x = F.relu (F.max_pool2d ( self.conv2(x), kernel_size =2, stride =2) )\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, 'loss train:', 0.036786358803510666, 'loss test:', 1.1606009006500244, ' accuracy test:', 0.800000011920929)\n",
      "(1, 'loss train:', 0.02580004185438156, 'loss test:', 1.195967674255371, ' accuracy test:', 0.800000011920929)\n",
      "(2, 'loss train:', 0.011333034373819828, 'loss test:', 1.2445627450942993, ' accuracy test:', 0.7900000214576721)\n",
      "(3, 'loss train:', 0.005387601908296347, 'loss test:', 1.294737696647644, ' accuracy test:', 0.7799999713897705)\n",
      "(4, 'loss train:', 0.0035095438361167908, 'loss test:', 1.3306933641433716, ' accuracy test:', 0.7900000214576721)\n",
      "(5, 'loss train:', 0.0026846681721508503, 'loss test:', 1.3568470478057861, ' accuracy test:', 0.7799999713897705)\n",
      "(6, 'loss train:', 0.0022211067844182253, 'loss test:', 1.377345323562622, ' accuracy test:', 0.7799999713897705)\n",
      "(7, 'loss train:', 0.0019191540777683258, 'loss test:', 1.3959357738494873, ' accuracy test:', 0.7799999713897705)\n",
      "(8, 'loss train:', 0.0016949467826634645, 'loss test:', 1.4131555557250977, ' accuracy test:', 0.7799999713897705)\n",
      "(9, 'loss train:', 0.0015209914417937398, 'loss test:', 1.4290831089019775, ' accuracy test:', 0.7799999713897705)\n"
     ]
    }
   ],
   "source": [
    "### train the model\n",
    "epoch = 1000\n",
    "batch_size = 20\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(),lr = 0.001,momentum = 0.5)\n",
    "i = 0\n",
    "for e in range(epoch):\n",
    "    for b in range(0,train_input.size(0),batch_size):\n",
    "        input_ = train_input.narrow(0,b,min(batch_size,train_input.size(0)-b))\n",
    "        target = train_target.narrow(0,b,min(batch_size,train_input.size(0)-b))\n",
    "        output = net(input_).view(min(batch_size,train_input.size(0)-b),2)\n",
    "        loss = criterion(output,target)\n",
    "        net.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    if e%100 == 0: # print the loss every 10 epochs\n",
    "        output = net(test_input).view(-1,2)\n",
    "        a,predicted_class = output.max(dim=1)\n",
    "        output_train = net(train_input).view(-1,2)\n",
    "        a,predicted_class_train = output.max(dim=1)\n",
    "        \n",
    "        #predicted_class = 1*(predicted_class.float().mean(dim=1) > 0.5)\n",
    "        print(e//100,'loss train:',criterion(output_train,train_target).data[0],'loss test:',criterion(output,test_target).data[0],' accuracy test:',accuracy(predicted_class,test_target).data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Augmentaiotn: Averaging two datapoints in the same class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "class0 = train_input[(train_target == 0).nonzero(),]\n",
    "class1 = train_input[(train_target == 1).nonzero(),]\n",
    "class0.squeeze_(dim=1)\n",
    "class1.squeeze_(dim=1)\n",
    "N = class0.shape[0]\n",
    "train_input_aug0 = [(class0[i]+class0[j])/2 for i in range(N) for j in range(i,N)]\n",
    "train_input_aug0 = torch.stack(train_input_aug0)\n",
    "train_out_aug0 = Variable(Tensor(train_input_aug0.shape[0]).fill_(0))\n",
    "N = class1.shape[0]\n",
    "train_input_aug1 = [(class1[i]+class1[j])/2 for i in range(N) for j in range(i,N)]\n",
    "train_input_aug1 = torch.stack(train_input_aug1)\n",
    "train_out_aug1 = Variable(Tensor(train_input_aug1.shape[0]).fill_(1))\n",
    "\n",
    "###\n",
    "train_aug = torch.cat((train_input_aug0,train_input_aug1))\n",
    "train_target_aug = torch.cat((train_out_aug0,train_out_aug1)).data.long()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Dataset:\n",
    "    def __init__(self,train_aug,train_target):\n",
    "        self.train_aug = train_aug.data\n",
    "        self.train_target = train_target\n",
    "    def __getitem__(self, index):\n",
    "        return self.train_aug[index], self.train_target[index]\n",
    "    def __len__(self):\n",
    "        return len(self.train_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data = Dataset(train_aug,train_target_aug)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dataset_loader = torch.utils.data.DataLoader(data,\n",
    "                                             batch_size = 40, shuffle=True,\n",
    "                                             num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "net = Net()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 loss train: 0.13896691799163818 loss test: 1.120018720626831  accuracy test: 0.7099999785423279\n",
      "1 loss train: 0.16479691863059998 loss test: 1.343389630317688  accuracy test: 0.7699999809265137\n",
      "2 loss train: 0.19951143860816956 loss test: 0.8837242722511292  accuracy test: 0.75\n",
      "3 loss train: 0.12719513475894928 loss test: 0.7926889061927795  accuracy test: 0.8299999833106995\n"
     ]
    }
   ],
   "source": [
    "### train the model\n",
    "epoch = 10\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(),lr = 0.01,momentum = 0.5)\n",
    "i = 0\n",
    "for e in range(epoch):\n",
    "    for input_, target_ in dataset_loader:\n",
    "        input_ = Variable(input_)\n",
    "        target_ = Variable(target_)\n",
    "        output = net(input_).view(-1,2)\n",
    "        loss = criterion(output,target_)\n",
    "        net.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    if e%1 == 0: # print the loss every 10 epochs\n",
    "        output = net(test_input).view(-1,2)\n",
    "        a,predicted_class = output.max(dim=1)\n",
    "        output_train = net(train_input).view(-1,2)\n",
    "        a,predicted_class_train = output.max(dim=1)\n",
    "        #predicted_class = 1*(predicted_class.float().mean(dim=1) > 0.5)\n",
    "        print(e//1,'loss train:',criterion(output_train,train_target).data[0],'loss test:',criterion(output,test_target).data[0],' accuracy test:',accuracy(predicted_class,test_target).data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [anaconda3]",
   "language": "python",
   "name": "Python [anaconda3]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
